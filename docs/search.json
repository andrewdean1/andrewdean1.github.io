[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my Machine Learning blog for Spring 2025!"
  },
  {
    "objectID": "posts/blog-1/blog1.html",
    "href": "posts/blog-1/blog1.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this analysis, I explore the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their physical characteristics. Using a decision tree classifier with carefully selected features including culmen length, flipper length, and island location, I achieved high accuracy in distinguishing between Adelie, Chinstrap, and Gentoo penguins. Through exploratory data analysis and feature selection, I identified the most important predictors and optimized the model’s depth parameter. The final model achieved excellent performance on the test set, with only one misclassification out of all test cases - a Gentoo penguin incorrectly identified as an Adelie. This demonstrates that penguin species can be reliably classified using a small set of physical measurements and location data.\n\n\nWe start by importing the Palmer Penguins dataset. Take a look at the first few rows to begin to understand the data.\n\n\nCode\n# Importing the Palmer Penguins dataset\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nData preparation is essential. Some steps that we took here are: 1. Dropping the columns that are not relevant to our analysis, and then drop any rows that contain missing values. 2. Converting the species labels into a numerical format that can be used by the model. 3. Converting the categorical features into dummy variables. Now take a look at the first few rows of the transformed data.\n\n\nCode\n# Data Prep\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNow that we have the data in a usable format, I started to explore the various features and their relationships. The goal of this is to guage which features might be best to use for our model.\n\n\nCode\n## Explore\n# 2 interesting visualizations\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# 1. Distribution of Culmen Length by Species\np1 = sns.scatterplot(x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", data = train, ax = axes[0])\n\n# 2. Spread of Flipper Length by Sex\np2 = sns.violinplot(x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Sex_FEMALE\", data = X_train, ax = axes[1])\np2.set_xlabel(\"Classified as Female\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove I’ve created two visualizations to explore the data. The first shows the relationship between flipper length and culmen length for each species. The second shows the distribution of flipper length for each sex. I created the first plot because I was curious whether 1 physical characteristic might help predict the magnitude of another (ie, if a penguin has a larger flipper, does it tend to have a larger culmen?). Second, I wanted to look at the distrubutions of a physical characteristic by sex. Based on the violin plot, there is lots of overlap in flipper length between males and females, which helps us rule out sex as potential leading predictor of flipper length. And, further, this fact can likely be spread to other physical characteristics, meaning sex might not help us predict physical characteristics (and thus species) at all.\n\n\nCode\n# Summary Table: compute the average or median value of some features, by group\n\ntrain.groupby(\"Species\").aggregate({\"Body Mass (g)\": [\"mean\", \"std\"], \"Culmen Length (mm)\": [\"mean\", \"std\"], \"Culmen Depth (mm)\": [\"mean\", \"std\"], \"Flipper Length (mm)\": [\"mean\", \"std\"]}).round(2)\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n3718.49\n462.66\n38.97\n2.64\n18.41\n1.22\n190.08\n6.69\n\n\nChinstrap penguin (Pygoscelis antarctica)\n3743.42\n407.42\n48.83\n3.45\n18.37\n1.14\n196.00\n7.42\n\n\nGentoo penguin (Pygoscelis papua)\n5039.95\n498.86\n47.07\n2.74\n14.91\n1.00\n216.75\n5.93\n\n\n\n\n\n\n\n\n\n\n\nHere I’ve created a summary table that shows the average and standard deviation of body mass, culmen length, culmen depth, and flipper length for each species. This helps us become aquainted with the magnitude of our data and its distribution. Looking at the standard deviations, we can see that the relative spread of each respective characteristic is similar across species. And, right off the bat, there is not a clear unique physical characteristic that can be used to identify each species. The Gentoo penguin does quite largely have the largest body mass, but then has a similar culmen length to the Chinstrap penguin. This is important to note because it suggests that we will need to utilize multiple physical characteristics to accurately predict species.\n\n\n\nFeature selection is likely the most imporant step in this analysis, as the entirety of our model’s performance will depend on the predictive power of our features. Below I use the SelectKBest algorithm to select the top 2 quantitative features and the top qualitative feature.\n\n\nCode\n## Features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Separate qualitataive and quantitative features\nqualitative_features = [\"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\nquantitative_features = [col for col in X_train.columns if col not in qualitative_features]\n\n# Select 2 quantitative features\nquant_selector = SelectKBest(f_classif, k = 2)\nX_train_quant = X_train[quantitative_features]\nquant_selector.fit(X_train_quant, y_train)\nX_new_train = quant_selector.transform(X_train_quant)\nselected_quant_features = X_train_quant.columns[quant_selector.get_support()]\n\n\n# Select 1 qualitataive features\nqual_selector = SelectKBest(f_classif, k = 3)\nX_train_qual = X_train[qualitative_features]\nqual_selector.fit(X_train_qual, y_train)\nX_new_train = qual_selector.transform(X_train_qual)\nselected_qual_feature = X_train_qual.columns[qual_selector.get_support()]\n\n# Combine selected quantitative and qualitative features\nfinal_features = list(selected_quant_features) + list(selected_qual_feature)\nX_new_train = X_train[final_features]\nprint(X_new_train.head(3))\n\n# Top 3 features are Culmen Length (mm)  Flipper Length (mm)  and Island.\n\n\n   Culmen Length (mm)  Flipper Length (mm)  Island_Biscoe  Island_Dream  \\\n0                40.9                187.0          False          True   \n1                49.0                210.0          False          True   \n2                50.0                218.0           True         False   \n\n   Island_Torgersen  \n0             False  \n1             False  \n2             False  \n\n\n\n\n\nI use DecisionTreeClassifier to create a baseline model. I utilize the top 3 features that we selected above to train the model and then evaluate its performance on the training set. We achieve 100% accuracy.\n\n\nCode\n# Initial model with top 3 features. Reached 100% accuracy on training set.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_new_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_new_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\n\n\nTraining Accuracy: 1.000\n\n\n\n\n\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 7, and this optimized model also achieves 100% accuracy on the training set.\n\n\nCode\n# Now to utilize the max_depth parameter to see if we can improve by finding the optimal depth\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 31)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_new_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_new_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_new_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\n\n\n\nOptimal Depth: 7\nTraining Accuracy with Optimal Depth: 1.000\n\n\n\n\n\nNow we evaluate the performance of our model on the test set. We achieve 98.5% accuracy, missing only 1 penguin.\n\n\nCode\n# Test model\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test_new = X_test[final_features]  # Use the same features selected for training\ntest_accuracy = dt_classifier_optimal.score(X_test_new, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\n\n\nTest Accuracy: 0.985\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(dt_classifier_optimal, X_new_train, y_train)\nplot_regions(dt_classifier_optimal, X_test_new, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two 3 ply plots show the predictions of our model on the training and test sets. The model is able to predict the species of the penguin with high accuracy on both sets by using flipper length, culmen length, and the island location. We can see the 1 missclassified penguin in Figure 5.1, shown by the red dot in the blue region.\n\n\n\nA confusion matrix is a great way to illustrate performance, showing the number of correct and incorrect predictions for each species. I created a plot to simply show this, where the shade of the cell represents the number of penguins who were classified as that species.\n\n\nCode\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = dt_classifier_optimal.predict(X_test_new)\nC = confusion_matrix(y_test, y_test_pred)\nC\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n# Enjoy a nice visualization in the form of a heatmap\nplt.figure(figsize=(10, 8))\n\n# heatmap\nsns.heatmap(C, \n            annot=True,  # show numbers in cells\n            fmt='d',     # use integer format\n            cmap='Purples',\n            xticklabels=le.classes_,  # use species names\n            yticklabels=le.classes_)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Species')\nplt.ylabel('True Species')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/blog-1/blog1.html#abstract",
    "href": "posts/blog-1/blog1.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this analysis, I explore the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their physical characteristics. Using a decision tree classifier with carefully selected features including culmen length, flipper length, and island location, I achieved high accuracy in distinguishing between Adelie, Chinstrap, and Gentoo penguins. Through exploratory data analysis and feature selection, I identified the most important predictors and optimized the model’s depth parameter. The final model achieved excellent performance on the test set, with only one misclassification out of all test cases - a Gentoo penguin incorrectly identified as an Adelie. This demonstrates that penguin species can be reliably classified using a small set of physical measurements and location data.\n\n\nWe start by importing the Palmer Penguins dataset. Take a look at the first few rows to begin to understand the data.\n\n\nCode\n# Importing the Palmer Penguins dataset\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nData preparation is essential. Some steps that we took here are: 1. Dropping the columns that are not relevant to our analysis, and then drop any rows that contain missing values. 2. Converting the species labels into a numerical format that can be used by the model. 3. Converting the categorical features into dummy variables. Now take a look at the first few rows of the transformed data.\n\n\nCode\n# Data Prep\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNow that we have the data in a usable format, I started to explore the various features and their relationships. The goal of this is to guage which features might be best to use for our model.\n\n\nCode\n## Explore\n# 2 interesting visualizations\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# 1. Distribution of Culmen Length by Species\np1 = sns.scatterplot(x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", data = train, ax = axes[0])\n\n# 2. Spread of Flipper Length by Sex\np2 = sns.violinplot(x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Sex_FEMALE\", data = X_train, ax = axes[1])\np2.set_xlabel(\"Classified as Female\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove I’ve created two visualizations to explore the data. The first shows the relationship between flipper length and culmen length for each species. The second shows the distribution of flipper length for each sex. I created the first plot because I was curious whether 1 physical characteristic might help predict the magnitude of another (ie, if a penguin has a larger flipper, does it tend to have a larger culmen?). Second, I wanted to look at the distrubutions of a physical characteristic by sex. Based on the violin plot, there is lots of overlap in flipper length between males and females, which helps us rule out sex as potential leading predictor of flipper length. And, further, this fact can likely be spread to other physical characteristics, meaning sex might not help us predict physical characteristics (and thus species) at all.\n\n\nCode\n# Summary Table: compute the average or median value of some features, by group\n\ntrain.groupby(\"Species\").aggregate({\"Body Mass (g)\": [\"mean\", \"std\"], \"Culmen Length (mm)\": [\"mean\", \"std\"], \"Culmen Depth (mm)\": [\"mean\", \"std\"], \"Flipper Length (mm)\": [\"mean\", \"std\"]}).round(2)\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n3718.49\n462.66\n38.97\n2.64\n18.41\n1.22\n190.08\n6.69\n\n\nChinstrap penguin (Pygoscelis antarctica)\n3743.42\n407.42\n48.83\n3.45\n18.37\n1.14\n196.00\n7.42\n\n\nGentoo penguin (Pygoscelis papua)\n5039.95\n498.86\n47.07\n2.74\n14.91\n1.00\n216.75\n5.93\n\n\n\n\n\n\n\n\n\n\n\nHere I’ve created a summary table that shows the average and standard deviation of body mass, culmen length, culmen depth, and flipper length for each species. This helps us become aquainted with the magnitude of our data and its distribution. Looking at the standard deviations, we can see that the relative spread of each respective characteristic is similar across species. And, right off the bat, there is not a clear unique physical characteristic that can be used to identify each species. The Gentoo penguin does quite largely have the largest body mass, but then has a similar culmen length to the Chinstrap penguin. This is important to note because it suggests that we will need to utilize multiple physical characteristics to accurately predict species.\n\n\n\nFeature selection is likely the most imporant step in this analysis, as the entirety of our model’s performance will depend on the predictive power of our features. Below I use the SelectKBest algorithm to select the top 2 quantitative features and the top qualitative feature.\n\n\nCode\n## Features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Separate qualitataive and quantitative features\nqualitative_features = [\"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\nquantitative_features = [col for col in X_train.columns if col not in qualitative_features]\n\n# Select 2 quantitative features\nquant_selector = SelectKBest(f_classif, k = 2)\nX_train_quant = X_train[quantitative_features]\nquant_selector.fit(X_train_quant, y_train)\nX_new_train = quant_selector.transform(X_train_quant)\nselected_quant_features = X_train_quant.columns[quant_selector.get_support()]\n\n\n# Select 1 qualitataive features\nqual_selector = SelectKBest(f_classif, k = 3)\nX_train_qual = X_train[qualitative_features]\nqual_selector.fit(X_train_qual, y_train)\nX_new_train = qual_selector.transform(X_train_qual)\nselected_qual_feature = X_train_qual.columns[qual_selector.get_support()]\n\n# Combine selected quantitative and qualitative features\nfinal_features = list(selected_quant_features) + list(selected_qual_feature)\nX_new_train = X_train[final_features]\nprint(X_new_train.head(3))\n\n# Top 3 features are Culmen Length (mm)  Flipper Length (mm)  and Island.\n\n\n   Culmen Length (mm)  Flipper Length (mm)  Island_Biscoe  Island_Dream  \\\n0                40.9                187.0          False          True   \n1                49.0                210.0          False          True   \n2                50.0                218.0           True         False   \n\n   Island_Torgersen  \n0             False  \n1             False  \n2             False  \n\n\n\n\n\nI use DecisionTreeClassifier to create a baseline model. I utilize the top 3 features that we selected above to train the model and then evaluate its performance on the training set. We achieve 100% accuracy.\n\n\nCode\n# Initial model with top 3 features. Reached 100% accuracy on training set.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_new_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_new_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\n\n\nTraining Accuracy: 1.000\n\n\n\n\n\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 7, and this optimized model also achieves 100% accuracy on the training set.\n\n\nCode\n# Now to utilize the max_depth parameter to see if we can improve by finding the optimal depth\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 31)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_new_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_new_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_new_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\n\n\n\nOptimal Depth: 7\nTraining Accuracy with Optimal Depth: 1.000\n\n\n\n\n\nNow we evaluate the performance of our model on the test set. We achieve 98.5% accuracy, missing only 1 penguin.\n\n\nCode\n# Test model\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test_new = X_test[final_features]  # Use the same features selected for training\ntest_accuracy = dt_classifier_optimal.score(X_test_new, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\n\n\nTest Accuracy: 0.985\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(dt_classifier_optimal, X_new_train, y_train)\nplot_regions(dt_classifier_optimal, X_test_new, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two 3 ply plots show the predictions of our model on the training and test sets. The model is able to predict the species of the penguin with high accuracy on both sets by using flipper length, culmen length, and the island location. We can see the 1 missclassified penguin in Figure 5.1, shown by the red dot in the blue region.\n\n\n\nA confusion matrix is a great way to illustrate performance, showing the number of correct and incorrect predictions for each species. I created a plot to simply show this, where the shade of the cell represents the number of penguins who were classified as that species.\n\n\nCode\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = dt_classifier_optimal.predict(X_test_new)\nC = confusion_matrix(y_test, y_test_pred)\nC\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n# Enjoy a nice visualization in the form of a heatmap\nplt.figure(figsize=(10, 8))\n\n# heatmap\nsns.heatmap(C, \n            annot=True,  # show numbers in cells\n            fmt='d',     # use integer format\n            cmap='Purples',\n            xticklabels=le.classes_,  # use species names\n            yticklabels=le.classes_)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Species')\nplt.ylabel('True Species')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/blog-1/blog1.html#discussion",
    "href": "posts/blog-1/blog1.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThis analysis demonstrates the effectiveness of using decision trees for penguin species classification. Through careful feature selection, we identified that just three key features - culmen length, flipper length, and island location - were sufficient to achieve nearly perfect classification accuracy. In identifying the most useful features by using the SelectKBest algorithm (2 quantitative, 1 qualitative), we were able to maximize our model efficiency and reach a high performance.\nThe exploratory data analysis revealed interesting patterns in penguin morphology. The scatter plots showed clear clustering of species based on physical characteristics, though with some overlap, particularly between Adelie and Chinstrap penguins. The violin plots examining sex differences in flipper length suggested that sex was not a strong predictor of physical characteristics, which helped inform our feature selection process.\nOur final model achieved high accuracy, misclassifying only one Gentoo penguin as an Adelie in the test set. This single error occurred on Biscoe Island, suggesting that while location is helpful for classification, it should not be relied upon exclusively. The confusion matrix visualization clearly showed this isolated error while highlighting the model’s otherwise perfect performance.\nOne key aspect that I learned from this project was the importance sufficient explortion, both in feature selection and model parameter tuning. It would be easy to pick 3 features that may look promising after pre-analyis plotting, but following a precise process to truly discover which features are optimal is critical and allows us to not solely rely on the confidence of our models based on their ultimate accuracy, but also know that we chose the best possible measurements of prediction. This extends to finding the optimal depth to use in the DecisionTreeClassifier model. Discovering the best depth to use adds another layer of assurance when relying on it to make predictions.\nFuture work could explore whether this high accuracy holds for larger datasets or different penguin populations. Are these results externally valid? Or are the features we chose reliant on the speciifc type of penguin that we measured in this blog post?Additionally, investigating whether simpler models (like logistic regression) could achieve similar performance might be worthwhile, potentially offering more interpretable results while maintaining accuracy."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nThis blog post explores the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their characteristics.\n\n\n\n\n\nFeb 19, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\nNo matching items"
  }
]