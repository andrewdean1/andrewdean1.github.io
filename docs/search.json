[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nDisccusses, considers, and critiques the the methods used to ensure fairness in ML algorithms.\n\n\n\n\n\nMar 26, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron\n\n\n\n\n\nDevelops perceptron algorithm from scratch.\n\n\n\n\n\nMar 24, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study\n\n\n\n\n\nReplicates the analysis of Obermeyer et al. (2019) on the effect of race on healthcare costs.\n\n\n\n\n\nMar 12, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nPredicts employment status. Gives comprehensive audit of bias.\n\n\n\n\n\nMar 11, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nDelve into a profit-driven loan approval model. Explore its ethical implications.\n\n\n\n\n\nMar 2, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nThis blog post explores the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their characteristics.\n\n\n\n\n\nFeb 19, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html",
    "href": "posts/bias-and-fairness/essay.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In today’s world, in which we are all driven by data, our personal activities and behaviors are continuously monitored through digital technologies. This information exists in various formats — whether it be in databases, lines of code in JSON, and taken from platforms such as social media. And. data processing continues to accelerate. Huge quantities of data are publicly accessible; enormous datasets can be acquired almost instantly through APIs, or requests can be made to get large amounts of data for analytical purposes. Machine learning algorithms consume these loads of data as direct pathways to generate decisions and suggestions for everyday users, often without their knowledge. Whether it be coorperations, government officials, or computer scientists, these models are widely used and have become an integral part of our lives. As algorithmic decision-making expanded into consequential domains, critics have identified biases against specific demographic groups. Notable examples that we have discussed include predictive policing algorithms and criminal recidivism tools that were demonstrated to exhibit significant bias against racial minorities. Because of this, it is crucial to employ metrics that assess algorithmic fairness. Today, various fairness verification frameworks are widely used, employing various quantitative methodologies, but this does not mean algorithms are thus definitively fair. Before we get into that, let’s discuss what fairness truly means in the machine learning realm."
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html#introduction",
    "href": "posts/bias-and-fairness/essay.html#introduction",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In today’s world, in which we are all driven by data, our personal activities and behaviors are continuously monitored through digital technologies. This information exists in various formats — whether it be in databases, lines of code in JSON, and taken from platforms such as social media. And. data processing continues to accelerate. Huge quantities of data are publicly accessible; enormous datasets can be acquired almost instantly through APIs, or requests can be made to get large amounts of data for analytical purposes. Machine learning algorithms consume these loads of data as direct pathways to generate decisions and suggestions for everyday users, often without their knowledge. Whether it be coorperations, government officials, or computer scientists, these models are widely used and have become an integral part of our lives. As algorithmic decision-making expanded into consequential domains, critics have identified biases against specific demographic groups. Notable examples that we have discussed include predictive policing algorithms and criminal recidivism tools that were demonstrated to exhibit significant bias against racial minorities. Because of this, it is crucial to employ metrics that assess algorithmic fairness. Today, various fairness verification frameworks are widely used, employing various quantitative methodologies, but this does not mean algorithms are thus definitively fair. Before we get into that, let’s discuss what fairness truly means in the machine learning realm."
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html#understanding-fairness",
    "href": "posts/bias-and-fairness/essay.html#understanding-fairness",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Understanding Fairness",
    "text": "Understanding Fairness\nThree primary quantitative fairness definitions exist. Error rate parity ensures all groups experience identical false negative and false positive rates - meaning algorithmic mistakes occur with equal frequency regardless of whether an individual belongs to Group A or Group B. Acceptance rate parity equalizes acceptance rates across all groups, ensuring algorithmic outcomes remain independent of group membership. Sufficiency requires that the probability of experiencing a positive outcome following a positive prediction remains consistent across all subgroups while similarly ensuring the probability of experiencing a negative outcome following a negative prediction remains uniform across subgroups. Another key notion, demographic parity, mandates equal selection rates across groups, aligning with the moral perspective of distributive justice by aiming to prevent systemic disadvantage. However, as Barocas, Hardt, and Narayanan (Barocas, Hardt, and Narayanan (2023)) note, this approach can overlook differences in qualification rates, raising questions about its applicability in ensuring true fairness.\nFrom a moral perspective, three viewpoints frame fairness considerations: narrow, middle, and broad. The narrow view maintains that individuals similar in task-relevant aspects deserve similar treatment, comparing all people as individuals rather than as group members. The middle view says that decision-makers must avoid perpetuating injustice by treating apparently dissimilar individuals similarly when their differences stem from problematic origins. The broad equality view aspires for individuals with comparable abilities and ambitions to achieve similar successes despite inevitable inequalities. This perspective transcends decision-making fairness to address the fundamental design of societal institutions, aiming to prevent unjust disparities from emerging initially (Barocas, Hardt, and Narayanan (2023)).\nA real-world example illustrating the challenges of quantitative fairness is the use of the COMPAS algorithm in the U.S. judicial system to predict recidivism. A study by ProPublica found that Black defendants were disproportionately labeled as higher risk compared to white defendants, despite similar reoffending rates (Angwin et al. (2016)). This case highlights the difficulties in achieving error rate parity, as false positive and false negative rates were not equal across racial groups. From a moral standpoint, this aligns with the middle fairness perspective, which emphasizes the importance of addressing systemic biases that lead to unjust outcomes (Barocas, Hardt, and Narayanan (2023))."
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html#quantitative-methods",
    "href": "posts/bias-and-fairness/essay.html#quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Quantitative Methods",
    "text": "Quantitative Methods\n​An illustrative example of employing both quantitative and moral frameworks to critique algorithmic bias is found in the study “Algorithmic Bias? An Empirical Study into Apparent Gender-Based Discrimination in Displaying STEM Career Ads” by Lambrecht and Tucker (Lambrecht and Tucker (2019)). This research investigates whether online advertising algorithms display STEM job advertisements differently based on gender. The authors conducted a field experiment by creating ads for STEM careers and observed their delivery across users. Their findings revealed that, even when controlling for factors like user behavior and advertiser intent, women were less likely to be shown these ads compared to men. The study suggests that this disparity arises not from explicit gender bias in the algorithms themselves but from underlying economic factors that lead to unintended, uneven outcomes. For instance, if women are less likely to click on STEM ads, the algorithm may optimize ad delivery towards men to maximize engagement, inadvertently perpetuating gender imbalances in STEM fields. This scenario demonstrates a failure to uphold the principle of demographic parity, where the likelihood of viewing a STEM career advertisement should be independent of gender. From a broader ethical standpoint, the study highlights the responsibility of platforms to ensure that their algorithms do not reinforce societal stereotypes or existing disparities. By critically analyzing and addressing these biases, the research underscores the importance of integrating both quantitative assessments and moral considerations to foster fairness in algorithmic decision-making.​"
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html#narayanans-view",
    "href": "posts/bias-and-fairness/essay.html#narayanans-view",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Narayanan’s View",
    "text": "Narayanan’s View\nQuantitative definitions of fairness alone are insufficient to exonerate an algorithm, argues Arvind Narayanan, a computer scientist and professor at Princeton University. In his October 11th, 2022 talk, titled “The Limits of the Quantitative Approach to Discrimination,” Narayanan presents a critical perspective on the overreliance on classical numerical fairness metrics. His key assertion highlights this: “Currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan (2022)).\nHe opens his speech with a well known quote: “all models are wrong, but some models are useful” (Narayanan (2022)), emphasizing that no machine learning model is perfectly accurate. The very process of simplifying complex realities into data-friendly trends inevitably introduces bias. By relying on these generalizations, models unintentionally reinforce the existing state of the world - a state already riddled with inequality and discrimination. Narayanan points out that data is far from neutral, stating, “data aren’t inert and objective. They are political, and produced towards certain ends” (Narayanan (2022)). Since data collection serves specific purposes, bias is inherently woven into the dataset from the start.\nThis idea is further elaborated in Fairness and Machine Learning: Limitations and Opportunities, co-authored by Narayanan, Barocas, and Hardt. The authors explain how the world’s complexities are distilled into rows, columns, and numerical values — an inherently flawed process. They challenge the notion of data as an objective snapshot of reality, arguing that “the term measurement is misleading, evoking an image of a dispassionate scientist recording what she observes, yet…it requires subjective human decisions” (Barocas, Hardt, and Narayanan (2023)).\nWhile people often believe data speaks for itself, Narayanan offers a more clear cut view: data is less an impartial truth and more a reflection of the systemic biases and inequalities that shape society. To counteract this, he advocates for a shift in focus: “we should be spending most of our time on curating and interrogating datasets before ever searching for statistical significance or fitting a model … it is important to look behind the facade of numbers to understand the hidden assumptions and politics of datasets” (Narayanan (2022)). This approach essentially reframes the role of data analysis, urging researchers to prioritize understanding the dataset’s origins and embedded biases rather than rushing to derive conclusions from potentially flawed numbers."
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html#the-quantitative-method-falling-short",
    "href": "posts/bias-and-fairness/essay.html#the-quantitative-method-falling-short",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Quantitative Method: Falling Short",
    "text": "The Quantitative Method: Falling Short\nArvind Narayanan’s critique underscores the limitations of relying solely on quantitative methods to assess algorithmic fairness. A pertinent example is found in the study “Ride-share matching algorithms generate income inequality” by Bokányi and Hannák (Bokányi and Hannák (2021)) This research examines how ride-sharing platforms’ matching algorithms may inadvertently contribute to income disparities among drivers. The authors developed a computational model simulating various factors such as driver and passenger locations, traffic conditions, and city layouts. Their simulations revealed that minor adjustments in system parameters could lead to significant income variations among drivers, even when their performance levels were identical. This suggests that the algorithm’s design choices can unpredictably affect income distribution, potentially reinforcing existing inequalities. While the statistical analyses in the study are robust, focusing solely on these quantitative metrics overlooks the broader ethical implications. The study does not sufficiently address the responsibility of platform designers to recognize and mitigate the reinforcement of societal inequities through algorithmic decisions. By concentrating exclusively on quantitative factors, the research misses an opportunity to explore how such algorithms might perpetuate systemic injustices, highlighting the need for a more holistic approach that integrates both quantitative analysis and ethical considerations.\nThis study exemplifies what D’Ignazio and Klein (D’Ignazio and Klein (2023)) term “Big Dick Data” in Data Feminism. They define this concept as big data projects driven by patriarchal, controlling ambitions of totalizing control through data collection and analysis. These projects are particularly harmful because they disregard context and overstate their technical and scientific authority. Data Feminism emphasizes the necessity of interrogating the the platforms under which data is gathered, an essential step that Ali et al. blatantly neglected."
  },
  {
    "objectID": "posts/bias-and-fairness/essay.html#conclusion",
    "href": "posts/bias-and-fairness/essay.html#conclusion",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Conclusion",
    "text": "Conclusion\nTo tie it back to Narayanan’s speech, he opens it up with a statement that should not be taken lightly: “I hope that this talk goes some way towards busting the myth that numbers don’t lie” (Narayanan (2022)). By the end, it’s clear that numbers, while often treated as objective truth, are shaped by the biases baked into the data they’re drawn from. This challenges the common belief that many, including myself, have in data as a driving force of truth. Through exploring these studies, it becomes evident that datasets are never fully neutral or detached from the world’s inequalities, and should thus all be taken from a critical perspective. As seen in the studies I have discussed, researchers can manipulate quantitative methods to validate their desired outcomes and frame the results in a way that is favorable to their argument.\nNarayanan’s argument pushes even further, claiming that quantitative methods often do more harm than good, reinforcing systemic biases under the notion of objectivity. While this criticism holds weight, I believe the solution isn’t to abandon quantitative methods altogether but to recognize their limitations and use them more thoughtfully. Quantitative metrics can still be powerful tools to uncover injustice if used responsibly. The key lies in avoiding reliance on any single definition of fairness. Since different definitions can yield conflicting conclusions, multiple perspectives must be considered to paint a fuller, more honest picture of algorithmic behavior. Moreover, numbers alone are never enough. Ethical frameworks, including the narrow, middle, and broad views of fairness, must guide how we interpret and apply these metrics. Only by blending quantitative analysis with moral reasoning can we hope to uncover and challenge the hidden biases within algorithmic systems."
  },
  {
    "objectID": "posts/blog-1/blog1.html",
    "href": "posts/blog-1/blog1.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this analysis, I explore the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their physical characteristics. Using a decision tree classifier with carefully selected features including culmen length, flipper length, and island location, I achieved high accuracy in distinguishing between Adelie, Chinstrap, and Gentoo penguins. Through exploratory data analysis and feature selection, I identified the most important predictors and optimized the model’s depth parameter. The final model achieved excellent performance on the test set, with only one misclassification out of all test cases - a Gentoo penguin incorrectly identified as an Adelie. This demonstrates that penguin species can be reliably classified using a small set of physical measurements and location data.\n\n\nWe start by importing the Palmer Penguins dataset. Take a look at the first few rows to begin to understand the data.\n\n\nCode\n# Importing the Palmer Penguins dataset\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nData preparation is essential. Some steps that we took here are: 1. Dropping the columns that are not relevant to our analysis, and then drop any rows that contain missing values. 2. Converting the species labels into a numerical format that can be used by the model. 3. Converting the categorical features into dummy variables. Now take a look at the first few rows of the transformed data.\n\n\nCode\n# Data Prep\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNow that we have the data in a usable format, I started to explore the various features and their relationships. The goal of this is to guage which features might be best to use for our model.\n\n\nCode\n## Explore\n# 2 interesting visualizations\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# 1. Distribution of Culmen Length by Species\np1 = sns.scatterplot(x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", data = train, ax = axes[0])\n\n# 2. Spread of Flipper Length by Sex\np2 = sns.violinplot(x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Sex_FEMALE\", data = X_train, ax = axes[1])\np2.set_xlabel(\"Classified as Female\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove I’ve created two visualizations to explore the data. The first shows the relationship between flipper length and culmen length for each species. The second shows the distribution of flipper length for each sex. I created the first plot because I was curious whether 1 physical characteristic might help predict the magnitude of another (ie, if a penguin has a larger flipper, does it tend to have a larger culmen?). Second, I wanted to look at the distrubutions of a physical characteristic by sex. Based on the violin plot, there is lots of overlap in flipper length between males and females, which helps us rule out sex as potential leading predictor of flipper length. And, further, this fact can likely be spread to other physical characteristics, meaning sex might not help us predict physical characteristics (and thus species) at all.\n\n\nCode\n# Summary Table: compute the average or median value of some features, by group\n\ntrain.groupby(\"Species\").aggregate({\"Body Mass (g)\": [\"mean\", \"std\"], \"Culmen Length (mm)\": [\"mean\", \"std\"], \"Culmen Depth (mm)\": [\"mean\", \"std\"], \"Flipper Length (mm)\": [\"mean\", \"std\"]}).round(2)\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n3718.49\n462.66\n38.97\n2.64\n18.41\n1.22\n190.08\n6.69\n\n\nChinstrap penguin (Pygoscelis antarctica)\n3743.42\n407.42\n48.83\n3.45\n18.37\n1.14\n196.00\n7.42\n\n\nGentoo penguin (Pygoscelis papua)\n5039.95\n498.86\n47.07\n2.74\n14.91\n1.00\n216.75\n5.93\n\n\n\n\n\n\n\n\n\n\n\nHere I’ve created a summary table that shows the average and standard deviation of body mass, culmen length, culmen depth, and flipper length for each species. This helps us become aquainted with the magnitude of our data and its distribution. Looking at the standard deviations, we can see that the relative spread of each respective characteristic is similar across species. And, right off the bat, there is not a clear unique physical characteristic that can be used to identify each species. The Gentoo penguin does quite largely have the largest body mass, but then has a similar culmen length to the Chinstrap penguin. This is important to note because it suggests that we will need to utilize multiple physical characteristics to accurately predict species.\n\n\n\nFeature selection is likely the most imporant step in this analysis, as the entirety of our model’s performance will depend on the predictive power of our features. Below I use the SelectKBest algorithm to select the top 2 quantitative features and the top qualitative feature.\n\n\nCode\n## Features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Separate qualitataive and quantitative features\nqualitative_features = [\"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\nquantitative_features = [col for col in X_train.columns if col not in qualitative_features]\n\n# Select 2 quantitative features\nquant_selector = SelectKBest(f_classif, k = 2)\nX_train_quant = X_train[quantitative_features]\nquant_selector.fit(X_train_quant, y_train)\nX_new_train = quant_selector.transform(X_train_quant)\nselected_quant_features = X_train_quant.columns[quant_selector.get_support()]\n\n\n# Select 1 qualitataive features\nqual_selector = SelectKBest(f_classif, k = 3)\nX_train_qual = X_train[qualitative_features]\nqual_selector.fit(X_train_qual, y_train)\nX_new_train = qual_selector.transform(X_train_qual)\nselected_qual_feature = X_train_qual.columns[qual_selector.get_support()]\n\n# Combine selected quantitative and qualitative features\nfinal_features = list(selected_quant_features) + list(selected_qual_feature)\nX_new_train = X_train[final_features]\nprint(X_new_train.head(3))\n\n# Top 3 features are Culmen Length (mm)  Flipper Length (mm)  and Island.\n\n\n   Culmen Length (mm)  Flipper Length (mm)  Island_Biscoe  Island_Dream  \\\n0                40.9                187.0          False          True   \n1                49.0                210.0          False          True   \n2                50.0                218.0           True         False   \n\n   Island_Torgersen  \n0             False  \n1             False  \n2             False  \n\n\n\n\n\nI use DecisionTreeClassifier to create a baseline model. I utilize the top 3 features that we selected above to train the model and then evaluate its performance on the training set. We achieve 100% accuracy.\n\n\nCode\n# Initial model with top 3 features. Reached 100% accuracy on training set.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_new_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_new_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\n\n\nTraining Accuracy: 1.000\n\n\n\n\n\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 7, and this optimized model also achieves 100% accuracy on the training set.\n\n\nCode\n# Now to utilize the max_depth parameter to see if we can improve by finding the optimal depth\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 31)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_new_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_new_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_new_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\n\n\n\nOptimal Depth: 7\nTraining Accuracy with Optimal Depth: 1.000\n\n\n\n\n\nNow we evaluate the performance of our model on the test set. We achieve 98.5% accuracy, missing only 1 penguin.\n\n\nCode\n# Test model\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test_new = X_test[final_features]  # Use the same features selected for training\ntest_accuracy = dt_classifier_optimal.score(X_test_new, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\n\n\nTest Accuracy: 0.985\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(dt_classifier_optimal, X_new_train, y_train)\nplot_regions(dt_classifier_optimal, X_test_new, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two 3 ply plots show the predictions of our model on the training and test sets. The model is able to predict the species of the penguin with high accuracy on both sets by using flipper length, culmen length, and the island location. We can see the 1 missclassified penguin in Figure 5.1, shown by the red dot in the blue region.\n\n\n\nA confusion matrix is a great way to illustrate performance, showing the number of correct and incorrect predictions for each species. I created a plot to simply show this, where the shade of the cell represents the number of penguins who were classified as that species.\n\n\nCode\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = dt_classifier_optimal.predict(X_test_new)\nC = confusion_matrix(y_test, y_test_pred)\nC\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n# Enjoy a nice visualization in the form of a heatmap\nplt.figure(figsize=(10, 8))\n\n# heatmap\nsns.heatmap(C, \n            annot=True,  # show numbers in cells\n            fmt='d',     # use integer format\n            cmap='Purples',\n            xticklabels=le.classes_,  # use species names\n            yticklabels=le.classes_)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Species')\nplt.ylabel('True Species')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/blog-1/blog1.html#abstract",
    "href": "posts/blog-1/blog1.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this analysis, I explore the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their physical characteristics. Using a decision tree classifier with carefully selected features including culmen length, flipper length, and island location, I achieved high accuracy in distinguishing between Adelie, Chinstrap, and Gentoo penguins. Through exploratory data analysis and feature selection, I identified the most important predictors and optimized the model’s depth parameter. The final model achieved excellent performance on the test set, with only one misclassification out of all test cases - a Gentoo penguin incorrectly identified as an Adelie. This demonstrates that penguin species can be reliably classified using a small set of physical measurements and location data.\n\n\nWe start by importing the Palmer Penguins dataset. Take a look at the first few rows to begin to understand the data.\n\n\nCode\n# Importing the Palmer Penguins dataset\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nData preparation is essential. Some steps that we took here are: 1. Dropping the columns that are not relevant to our analysis, and then drop any rows that contain missing values. 2. Converting the species labels into a numerical format that can be used by the model. 3. Converting the categorical features into dummy variables. Now take a look at the first few rows of the transformed data.\n\n\nCode\n# Data Prep\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNow that we have the data in a usable format, I started to explore the various features and their relationships. The goal of this is to guage which features might be best to use for our model.\n\n\nCode\n## Explore\n# 2 interesting visualizations\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# 1. Distribution of Culmen Length by Species\np1 = sns.scatterplot(x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", data = train, ax = axes[0])\n\n# 2. Spread of Flipper Length by Sex\np2 = sns.violinplot(x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Sex_FEMALE\", data = X_train, ax = axes[1])\np2.set_xlabel(\"Classified as Female\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove I’ve created two visualizations to explore the data. The first shows the relationship between flipper length and culmen length for each species. The second shows the distribution of flipper length for each sex. I created the first plot because I was curious whether 1 physical characteristic might help predict the magnitude of another (ie, if a penguin has a larger flipper, does it tend to have a larger culmen?). Second, I wanted to look at the distrubutions of a physical characteristic by sex. Based on the violin plot, there is lots of overlap in flipper length between males and females, which helps us rule out sex as potential leading predictor of flipper length. And, further, this fact can likely be spread to other physical characteristics, meaning sex might not help us predict physical characteristics (and thus species) at all.\n\n\nCode\n# Summary Table: compute the average or median value of some features, by group\n\ntrain.groupby(\"Species\").aggregate({\"Body Mass (g)\": [\"mean\", \"std\"], \"Culmen Length (mm)\": [\"mean\", \"std\"], \"Culmen Depth (mm)\": [\"mean\", \"std\"], \"Flipper Length (mm)\": [\"mean\", \"std\"]}).round(2)\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n3718.49\n462.66\n38.97\n2.64\n18.41\n1.22\n190.08\n6.69\n\n\nChinstrap penguin (Pygoscelis antarctica)\n3743.42\n407.42\n48.83\n3.45\n18.37\n1.14\n196.00\n7.42\n\n\nGentoo penguin (Pygoscelis papua)\n5039.95\n498.86\n47.07\n2.74\n14.91\n1.00\n216.75\n5.93\n\n\n\n\n\n\n\n\n\n\n\nHere I’ve created a summary table that shows the average and standard deviation of body mass, culmen length, culmen depth, and flipper length for each species. This helps us become aquainted with the magnitude of our data and its distribution. Looking at the standard deviations, we can see that the relative spread of each respective characteristic is similar across species. And, right off the bat, there is not a clear unique physical characteristic that can be used to identify each species. The Gentoo penguin does quite largely have the largest body mass, but then has a similar culmen length to the Chinstrap penguin. This is important to note because it suggests that we will need to utilize multiple physical characteristics to accurately predict species.\n\n\n\nFeature selection is likely the most imporant step in this analysis, as the entirety of our model’s performance will depend on the predictive power of our features. Below I use the SelectKBest algorithm to select the top 2 quantitative features and the top qualitative feature.\n\n\nCode\n## Features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Separate qualitataive and quantitative features\nqualitative_features = [\"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\nquantitative_features = [col for col in X_train.columns if col not in qualitative_features]\n\n# Select 2 quantitative features\nquant_selector = SelectKBest(f_classif, k = 2)\nX_train_quant = X_train[quantitative_features]\nquant_selector.fit(X_train_quant, y_train)\nX_new_train = quant_selector.transform(X_train_quant)\nselected_quant_features = X_train_quant.columns[quant_selector.get_support()]\n\n\n# Select 1 qualitataive features\nqual_selector = SelectKBest(f_classif, k = 3)\nX_train_qual = X_train[qualitative_features]\nqual_selector.fit(X_train_qual, y_train)\nX_new_train = qual_selector.transform(X_train_qual)\nselected_qual_feature = X_train_qual.columns[qual_selector.get_support()]\n\n# Combine selected quantitative and qualitative features\nfinal_features = list(selected_quant_features) + list(selected_qual_feature)\nX_new_train = X_train[final_features]\nprint(X_new_train.head(3))\n\n# Top 3 features are Culmen Length (mm)  Flipper Length (mm)  and Island.\n\n\n   Culmen Length (mm)  Flipper Length (mm)  Island_Biscoe  Island_Dream  \\\n0                40.9                187.0          False          True   \n1                49.0                210.0          False          True   \n2                50.0                218.0           True         False   \n\n   Island_Torgersen  \n0             False  \n1             False  \n2             False  \n\n\n\n\n\nI use DecisionTreeClassifier to create a baseline model. I utilize the top 3 features that we selected above to train the model and then evaluate its performance on the training set. We achieve 100% accuracy.\n\n\nCode\n# Initial model with top 3 features. Reached 100% accuracy on training set.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_new_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_new_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\n\n\nTraining Accuracy: 1.000\n\n\n\n\n\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 7, and this optimized model also achieves 100% accuracy on the training set.\n\n\nCode\n# Now to utilize the max_depth parameter to see if we can improve by finding the optimal depth\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 31)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_new_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_new_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_new_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\n\n\n\nOptimal Depth: 7\nTraining Accuracy with Optimal Depth: 1.000\n\n\n\n\n\nNow we evaluate the performance of our model on the test set. We achieve 98.5% accuracy, missing only 1 penguin.\n\n\nCode\n# Test model\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test_new = X_test[final_features]  # Use the same features selected for training\ntest_accuracy = dt_classifier_optimal.score(X_test_new, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\n\n\nTest Accuracy: 0.985\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(dt_classifier_optimal, X_new_train, y_train)\nplot_regions(dt_classifier_optimal, X_test_new, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two 3 ply plots show the predictions of our model on the training and test sets. The model is able to predict the species of the penguin with high accuracy on both sets by using flipper length, culmen length, and the island location. We can see the 1 missclassified penguin in Figure 5.1, shown by the red dot in the blue region.\n\n\n\nA confusion matrix is a great way to illustrate performance, showing the number of correct and incorrect predictions for each species. I created a plot to simply show this, where the shade of the cell represents the number of penguins who were classified as that species.\n\n\nCode\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = dt_classifier_optimal.predict(X_test_new)\nC = confusion_matrix(y_test, y_test_pred)\nC\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n# Enjoy a nice visualization in the form of a heatmap\nplt.figure(figsize=(10, 8))\n\n# heatmap\nsns.heatmap(C, \n            annot=True,  # show numbers in cells\n            fmt='d',     # use integer format\n            cmap='Purples',\n            xticklabels=le.classes_,  # use species names\n            yticklabels=le.classes_)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Species')\nplt.ylabel('True Species')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/blog-1/blog1.html#discussion",
    "href": "posts/blog-1/blog1.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThis analysis demonstrates the effectiveness of using decision trees for penguin species classification. Through careful feature selection, we identified that just three key features - culmen length, flipper length, and island location - were sufficient to achieve nearly perfect classification accuracy. In identifying the most useful features by using the SelectKBest algorithm (2 quantitative, 1 qualitative), we were able to maximize our model efficiency and reach a high performance.\nThe exploratory data analysis revealed interesting patterns in penguin morphology. The scatter plots showed clear clustering of species based on physical characteristics, though with some overlap, particularly between Adelie and Chinstrap penguins. The violin plots examining sex differences in flipper length suggested that sex was not a strong predictor of physical characteristics, which helped inform our feature selection process.\nOur final model achieved high accuracy, misclassifying only one Gentoo penguin as an Adelie in the test set. This single error occurred on Biscoe Island, suggesting that while location is helpful for classification, it should not be relied upon exclusively. The confusion matrix visualization clearly showed this isolated error while highlighting the model’s otherwise perfect performance.\nOne key aspect that I learned from this project was the importance sufficient explortion, both in feature selection and model parameter tuning. It would be easy to pick 3 features that may look promising after pre-analyis plotting, but following a precise process to truly discover which features are optimal is critical and allows us to not solely rely on the confidence of our models based on their ultimate accuracy, but also know that we chose the best possible measurements of prediction. This extends to finding the optimal depth to use in the DecisionTreeClassifier model. Discovering the best depth to use adds another layer of assurance when relying on it to make predictions.\nFuture work could explore whether this high accuracy holds for larger datasets or different penguin populations. Are these results externally valid? Or are the features we chose reliant on the speciifc type of penguin that we measured in this blog post?Additionally, investigating whether simpler models (like logistic regression) could achieve similar performance might be worthwhile, potentially offering more interpretable results while maintaining accuracy."
  },
  {
    "objectID": "posts/blog-2/blog-2.html",
    "href": "posts/blog-2/blog-2.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Introduction\nThis blog tackles a real-world problem: given information about an individual and their specific loan request, should a bank offer this person a loan? I conducted an exploration of the data to determine which features are the best predictors of loan status (whether someone will default or not). Using logistic regression, I determined that person_home_ownership and loan_percent_income were the most predictive features and created a weight vector for a linear score function.\nWith the goal of maximizing bank profit, I established an optimal threshold value: individuals with scores below this threshold are hypothetically offered loans, while those above are denied. However, as shown in my analysis, a model created solely to maximize profit can lead to disparate impacts across different groups, which will be discussed more at the conclusion of this post. The approval rates vary by age (with older applicants facing much lower approval rates), loan purpose (with medical loans having the lowest approval rates), and income level (with a clear advantage for higher-income applicants).\nWhile the model is effective at predicting defaults and maximizing profits, these findings raise important questions about fairness and access to credit that financial institutions should consider beyond pure profit maximization.\n\n\nCode\n# Import relevant packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\n\n\nGrab the Data\n\n\nCode\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train.dropna()\ndf_train\ndf_train = df_train[df_train['person_age'] &lt;= 100]\n\n\n\n\nExplore the Data\n\n\nCode\n# Quick summary table\nsummary_stats = df_train.groupby(['person_home_ownership']).agg({\n    'loan_amnt': ['mean', 'std'],\n    'person_income': ['mean', 'std'],\n    'person_age': ['mean', 'std'],\n    'loan_percent_income': ['mean', 'std'],\n    'loan_int_rate': ['mean', 'std']\n})\n\nsummary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\nsummary_stats = summary_stats.reset_index()\n\nsummary_stats = summary_stats.rename(columns={\n    'loan_amnt_mean': 'Loan Amount (Mean)',\n    'loan_amnt_std': 'Loan Amount (Std)',\n    'person_income_mean': 'Income (Mean)',\n    'person_income_std': 'Income (Std)',\n    'person_age_mean': 'Age (Mean)',\n    'person_age_std': 'Age (Std)',\n    'loan_percent_income_mean': 'Loan % Income (Mean)',\n    'loan_percent_income_std': 'Loan % Income (Std)',\n    'loan_int_rate_mean': 'Interest Rate (Mean)',\n    'loan_int_rate_std': 'Interest Rate (Std)'\n})\n\nfor col in summary_stats.columns:\n    if col != 'person_home_ownership':\n        summary_stats[col] = summary_stats[col].round(2)\nsummary_stats\n\n\n\n\n\n\n\n\n\nperson_home_ownership\nLoan Amount (Mean)\nLoan Amount (Std)\nIncome (Mean)\nIncome (Std)\nAge (Mean)\nAge (Std)\nLoan % Income (Mean)\nLoan % Income (Std)\nInterest Rate (Mean)\nInterest Rate (Std)\n\n\n\n\n0\n0\n11305.19\n6138.49\n82145.16\n92740.81\n27.22\n6.58\n0.19\n0.11\n12.06\n2.71\n\n\n1\n1\n8912.19\n5797.05\n55567.60\n39786.38\n27.52\n6.32\n0.18\n0.11\n11.45\n3.12\n\n\n2\n2\n10620.18\n6779.92\n81684.78\n84164.30\n28.01\n6.40\n0.15\n0.10\n10.53\n3.30\n\n\n3\n3\n9051.96\n6218.84\n58930.36\n53365.47\n27.67\n6.07\n0.18\n0.11\n10.95\n3.23\n\n\n\n\n\n\n\nTable 1. Displays summary statistics for some key variables grouped by home ownership status. Home ownership classification: 0 = Other, 1 = Rent, 2 = Mortgage, 3 = Own.\n\n\nCode\n# Visualization #1\nplt.figure(figsize=(12, 6))\n\nsns.violinplot(data=df_train, \n               x='loan_intent', \n               y='person_age',\n               hue='loan_intent',\n               palette='viridis',\n               inner='box') \n\nplt.title('Age Distribution by Loan Intent', pad=15, fontsize=14)\nplt.xlabel('Loan Purpose', fontsize=12)\nplt.ylabel('Age', fontsize=12)\n\nplt.xticks(rotation=30)\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 1. Shows how the distributions of loan intent vary by age. Medical and personal loans have the largest age spread and education seems to have the lowest median age.\n\n\nCode\n# Visualization #2\n# Create income categories for better visualization\ndf_train['income_category'] = pd.qcut(df_train['person_income'], \n                                    q=5, \n                                    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\nplt.figure(figsize=(12, 6))\n\nsns.boxplot(data=df_train, \n           x='income_category', \n           y='loan_amnt',\n           hue='person_home_ownership',\n           palette='deep')\n\nplt.title('Loan Amounts by Income Level and Home Ownership', pad=15, fontsize=14)\nplt.xlabel('Income Category', fontsize=12)\nplt.ylabel('Loan Amount ($)', fontsize=12)\n\n# format y-axis to show thousands\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(title='Home Ownership', bbox_to_anchor=(1.05, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 2. Shows loan amounts across different income levels and home ownership status, which can help us understand who gets access to larger loans. I created 5 income brackets (separated the income category into 5 categories) to help show which segments of perspective borrowers have access to larger loans. This gives us perspective on the magnitude of loans, giving us some insight into who might be more likely to default (for example, maybe the outliers in the “Very Low” bracket are likely to default, as they are quite largely above the median loan amount for the rest of their bracket).\n\n\nBuild a Model\nThe code below prepares our loan data for modeling by converting text categories into numbers that our algorithm can understand. We use a label encoder to transform loan intent and credit history into numerical values, manually convert home ownership types into a numbered scale (0-3), and separate our data into features (X_train) and the target we want to predict (y_train, which indicates whether loans were repaid or defaulted). This preprocessing step is essential before we can build our predictive model.\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_train = df_train.drop(columns=['income_category'])\n\nlabel_encoder = LabelEncoder()\n\nqual_features = [\n    'loan_intent',\n    'cb_person_default_on_file'\n]\n\nfor label in qual_features:\n    df_train[label] = label_encoder.fit_transform(df_train[label])\n\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OTHER', 0)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('RENT', 1)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('MORTGAGE', 2)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OWN', 3)\n\ny_train = df_train[\"loan_status\"]\nX_train = df_train.drop(columns=['loan_status', 'loan_grade'])\nX_train\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\n1\n3.0\n1\n11750\n13.47\n0.12\n1\n6\n\n\n2\n22\n36996\n1\n5.0\n1\n10000\n7.51\n0.27\n0\n4\n\n\n3\n24\n26000\n1\n2.0\n3\n1325\n12.87\n0.05\n0\n4\n\n\n4\n29\n53004\n2\n2.0\n2\n15000\n9.63\n0.28\n0\n10\n\n\n6\n21\n21700\n1\n2.0\n2\n5500\n14.91\n0.25\n0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n2\n8.0\n1\n3000\n7.29\n0.02\n0\n17\n\n\n26060\n23\n48000\n1\n1.0\n5\n4325\n5.42\n0.09\n0\n4\n\n\n26061\n22\n60000\n1\n0.0\n3\n15000\n11.71\n0.25\n0\n4\n\n\n26062\n30\n144000\n2\n12.0\n4\n35000\n12.68\n0.24\n0\n8\n\n\n26063\n25\n60000\n1\n5.0\n1\n21450\n7.29\n0.36\n0\n4\n\n\n\n\n22907 rows × 10 columns\n\n\n\n\n\nFind best features\nBelow I work to find the best features for our logistic regression model.\nThis code systematically tests all possible pairs of features to find which combination best predicts loan defaults. For each pair, I train a logistic regression model, evaluate its performance using cross-validation, and store the results. The code then ranks these combinations by their prediction accuracy, revealing that person_home_ownership and loan_percent_income are the most powerful predictors for determining whether someone will repay their loan.\nI experimented with different numbers of features, and found that 2 features yielded the best results. I fit the logistic regression model with a cross-validation of 5 folds for each combination of features.\n\n\nCode\n# Find best features\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\npd.set_option('max_colwidth', 10000)\nnum_features = 2\n\ncols = X_train.columns\n\nscore_df_columns = ['features', 'score', 'weights']\nscore_df = pd.DataFrame(columns = score_df_columns)\n\nfor features in combinations(cols, num_features):\n  features = list(features)\n  \n  LR = LogisticRegression(max_iter = 10000000000)\n  LR.fit(X_train[features], y_train)  \n  LRscore = cross_val_score(LR, X_train[features], y_train, cv = 5).mean()\n  score_df.loc[len(score_df.index)] = [features, LRscore, LR.coef_]  \n\nscore_df = score_df.sort_values(by='score', ascending=False).head(5)\nscore_df\n\n\n\n\n\n\n\n\n\nfeatures\nscore\nweights\n\n\n\n\n21\n[person_home_ownership, loan_percent_income]\n0.848736\n[[-1.009688163139443, 8.271969139654322]]\n\n\n36\n[loan_amnt, loan_percent_income]\n0.824071\n[[-6.038304758244034e-05, 10.192521499044894]]\n\n\n39\n[loan_int_rate, loan_percent_income]\n0.823373\n[[0.28965363840490665, 8.41798717832072]]\n\n\n14\n[person_income, loan_percent_income]\n0.822238\n[[-1.0897117986924622e-05, 7.210498494849672]]\n\n\n32\n[loan_intent, loan_percent_income]\n0.819269\n[[-0.11082528287162276, 8.256012358077584]]\n\n\n\n\n\n\n\nAs seen by the table above, the best two features are person_home_ownership and loan_percent_income. Now we can define our weights from these features.\n\n\nCode\n# create weight array\nw = score_df.iloc[0][\"weights\"][0]\nw\n\n\narray([-1.00968816,  8.27196914])\n\n\n\n\nFind a Threshold\nBelow I build a loan approval model using the two most predictive features and calculate potential profits for the bank. I create functions to estimate earnings from successfully repaid loans (with 10 years of interest) and losses from defaulted loans (with only 3 years of interest plus a 70% loss of principal). These calculations will help determine the optimal threshold for approving loans that maximizes the bank’s profits.\n\n\nCode\n# Start by defining X_train with the features found above\nX_train = X_train[['person_home_ownership', 'loan_percent_income']]\n\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\n\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(X_train, w)\n\n# Calculate profit for repaid loans \ndef profit_repaid(loan_amount, interest_rate):\n    return loan_amount * (1 + 0.25 * interest_rate)**10 - loan_amount\n\n# Calculate profit for defaulted loans\ndef profit_defaulted(loan_amount, interest_rate):\n    return loan_amount * (1 + 0.25 * interest_rate)**3 - 1.7 * loan_amount\n\n\nOur manipulated data, meaning we only kept the two most predictive features:\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n\n\n\nperson_home_ownership\nloan_percent_income\n\n\n\n\n1\n1\n0.12\n\n\n2\n1\n0.27\n\n\n3\n1\n0.05\n\n\n4\n2\n0.28\n\n\n6\n1\n0.25\n\n\n\n\n\n\n\n\n\nFind the optimal threshold\nThe code below finds the optimal threshold for loan approval by testing 100 different cutoff points between 0 and 1. For each threshold, I calculate the expected profit by approving loans below that threshold (predicted to be repaid) and rejecting those above it (predicted to default). I then identify the profit-maximizing threshold, visualizing the relationship between thresholds and profits, and reporting key metrics including the expected profit per borrower and the overall loan approval rate.\n\n\nCode\n# Finding the optimal threshold\nthresholds = np.linspace(0, 1, 100)\nprofits_per_borrower = []\n\n# Get predicted probabilities from our model\n# y_prob_train has the predicted probability of default for each loan in training set\ny_prob_train = model.predict_proba(X_train)[:, 1] \n\nfor threshold in thresholds:\n    y_pred = (y_prob_train &gt;= threshold)\n\n    loan_amounts = df_train.loc[X_train.index, 'loan_amnt'] #get the loan amount for each loan in training set\n    interest_rates = df_train.loc[X_train.index, 'loan_int_rate'] / 100 #convert to decimal\n\n    # Calculate total profit (gains/losses)\n    # For predicted repaid loans (y_pred == 0): we use profits_repaid\n    # For predicted defaults (y_pred == 1): deny the loan, so profit is 0\n    profits_repaid = profit_repaid(loan_amounts, interest_rates)\n    profits_defaulted = profit_defaulted(loan_amounts, interest_rates)\n    \n    predicted_repay = (y_pred == 0)\n    \n    # only give loans to those predicted to repay\n    # for those predicted to default, we don't give loans, so profit is 0\n    total_profit = np.sum(profits_repaid * predicted_repay)\n    \n    # calculate what actually happens - some loans we approve will default (doesn't affect our decision making tho)\n    actual_defaults = (y_train == 1)\n    actual_repayments = (y_train == 0)\n    \n    # Calculate what we actually earn considering who actually defaults and who repays\n    # Among approved loans (predicted_repay):\n    # thosewho actually repay (actual_repayments): we get profits_repaid\n    # those who actually default (actual_defaults): we get profits_defaulted\n    actual_profit = np.sum(profits_repaid * (predicted_repay & actual_repayments)) + \\\n                    np.sum(profits_defaulted * (predicted_repay & actual_defaults))\n    \n    profit_per_borrower = actual_profit / len(X_train)\n    profits_per_borrower.append(profit_per_borrower)\n\n# get optimal threshold\noptimal_threshold = thresholds[np.argmax(profits_per_borrower)]\nmax_profit_per_borrower = np.max(profits_per_borrower)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, profits_per_borrower)\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', \n            label=f'Optimal threshold = {optimal_threshold:.3f}')\nplt.title('Profit per Borrower vs Classification Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Profit per Borrower ($)')\nplt.grid(False)\nplt.legend()\nplt.show()\n\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")\nprint(f\"Expected profit per borrower: ${max_profit_per_borrower:,.2f}\")\n\n# show confusion matrix at optimal threshold\ny_pred_optimal = (y_prob_train &gt;= optimal_threshold)\nprint(\"\\nConfusion Matrix at Optimal Threshold:\")\nprint(confusion_matrix(y_train, y_pred_optimal))\n\n# show approval rate\napproval_rate = np.mean(y_pred_optimal == 0) * 100\nprint(f\"\\nApproval rate: {approval_rate:.1f}%\")\n\n\n\n\n\n\n\n\n\n\nOptimal threshold: 0.535\nExpected profit per borrower: $1,450.63\n\nConfusion Matrix at Optimal Threshold:\n[[17863   118]\n [ 3253  1673]]\n\nApproval rate: 92.2%\n\n\nFigure 3. Shows the optimal threshold (profit maximizing) profit per borrower.\n\n\nEvaluate the Model from the Bank’s Perspective\n\n\nCode\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test =df_test.dropna()\n\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OTHER', 0)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('RENT', 1)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('MORTGAGE', 2)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OWN', 3)\n\nX_test = df_test[['person_home_ownership', 'loan_percent_income']]\n\n\nBelow I evaluate our loan model on new testing data, calculating approval rates and expected profits. It applies our optimal threshold to predict which loans to approve, estimates potential earnings from repaid loans and losses from defaults, and generates a performance report with key metrics. I also create a histogram showing the distribution of default probabilities across all applicants, with a vertical line marking our chosen approval threshold, helping us visualize how our model makes decisions.\n\n\nCode\n\ny_prob_test = model.predict_proba(X_test)[:, 1]\n\ny_pred_test = (y_prob_test &gt;= optimal_threshold)\n\nloan_amounts_test = df_test['loan_amnt']\ninterest_rates_test = df_test['loan_int_rate'] / 100\n\n# Calculate potential profits for test loans\nprofits_repaid_test = profit_repaid(loan_amounts_test, interest_rates_test)\nprofits_defaulted_test = profit_defaulted(loan_amounts_test, interest_rates_test)\n\n# Determine which loans we would approve\npredicted_repay_test = (y_pred_test == 0)\n\n# Calculate approval rate on test set\napproval_rate_test = np.mean(predicted_repay_test) * 100\nprint(f\"Test set approval rate: {approval_rate_test:.1f}%\")\n\n# If test set has actual outcomes (y_test), get performance\nif 'loan_status' in df_test.columns:\n    y_test = df_test['loan_status']\n    \n    # confusion matrix\n    print(\"\\nConfusion Matrix on Test Set:\")\n    print(confusion_matrix(y_test, y_pred_test))\n    \n    # calculate classification metrics\n    print(\"\\nClassification Report on Test Set:\")\n    print(classification_report(y_test, y_pred_test))\n    \n    # calculate actual profit on test set\n    actual_defaults_test = (y_test == 1)\n    actual_repayments_test = (y_test == 0)\n    \n    actual_profit_test = np.sum(profits_repaid_test * (predicted_repay_test & actual_repayments_test)) + \\\n                         np.sum(profits_defaulted_test * (predicted_repay_test & actual_defaults_test))\n    \n    profit_per_borrower_test = actual_profit_test / len(X_test)\n    print(f\"\\nExpected profit per borrower on test set: ${profit_per_borrower_test:,.2f}\")\nelse:\n    # If we don't have actual outcomes, just calculate expected profit based on model predictions\n    expected_profit = 0\n    for i, prob in enumerate(y_prob_test):\n        # Expected profit = (probability of repayment × profit if repaid) + \n        #                   (probability of default × profit if defaulted)\n        if predicted_repay_test[i]:  # If we approve this loan\n            expected_profit += (1-prob) * profits_repaid_test.iloc[i] + prob * profits_defaulted_test.iloc[i]\n    \n    expected_profit_per_borrower = expected_profit / len(X_test)\n    print(f\"\\nExpected profit per borrower on test set (based on model probabilities): ${expected_profit_per_borrower:,.2f}\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(y_prob_test, bins=50, alpha=0.7)\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', \n            label=f'Optimal threshold = {optimal_threshold:.3f}')\nplt.title('Distrubution of the Default Probabilities in Test Set')\nplt.xlabel('Predicted Probability of Default')\nplt.ylabel('Number of Loans')\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\nTest set approval rate: 91.6%\n\nConfusion Matrix on Test Set:\n[[4427   27]\n [ 824  453]]\n\nClassification Report on Test Set:\n              precision    recall  f1-score   support\n\n           0       0.84      0.99      0.91      4454\n           1       0.94      0.35      0.52      1277\n\n    accuracy                           0.85      5731\n   macro avg       0.89      0.67      0.71      5731\nweighted avg       0.87      0.85      0.82      5731\n\n\nExpected profit per borrower on test set: $1,394.87\n\n\n\n\n\n\n\n\n\nThe expected profit per borrower is $1,394.87. This is about $50 lower than the profit per borrower we predicted from the training set. The plot above shows the distribution of the predicted probabilities of defaulting in the test set and the number of loans given for each predicted probability. This is useful for understanding the ultimate duty of my model: as the probability of defaulting increases, we see the magnitude of loans given decreasing.\n\n\nEvaluate the Model From the Borrower’s Perspective\n\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\nTo answer this question, I first created five age brackets (under 30, 30-50, 50-70, 70-90, and over 90) and calculated the approval rate for each group. I then visualized these rates in a bar chart. The results reveal a concerning pattern: while younger applicants have similar approval chances, there’s a sharp decline for those over 70, with virtually no approvals for applicants over 90, highlighting potential age-based disparities in credit access.\n\n\nCode\n# create age groups\nbins = [0, 30, 50, 70, 90, 150]\nlabels= [\"&lt;30\", \"30-50\", \"50-70\", \"70-90\", \"90+\"]\n\ndf_test['person_age_group'] = pd.cut(df_test[\"person_age\"], bins=bins, labels=labels, right=False)\n\ndf_test['expect_default'] = np.where(y_prob_test &gt;= optimal_threshold, 1, 0)\ndf_test['approved'] = 1 - df_test['expect_default']\n\napproval_by_age = df_test.groupby('person_age_group')['approved'].mean()\n\nprint(\"Approval rates by age group:\")\nprint(approval_by_age)\n\nplt.figure(figsize=(10, 6))\napproval_by_age.plot(kind='bar', color='skyblue')\nplt.title('Loan Approval Rates by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Approval Rate (%)')\nplt.legend()\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n\n\nApproval rates by age group:\nperson_age_group\n&lt;30      0.911374\n30-50    0.930900\n50-70    0.888889\n70-90    0.500000\n90+           NaN\nName: approved, dtype: float64\n\n\n\n\n\n\n\n\n\nFigure 4. Shows the approval rates by age group.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"Distribution of Loans by Age\")\nsns.histplot(data = df_test, x = \"person_age\", hue = \"expect_default\", bins = 30).set(xlabel = \"age\", ylabel = \"count\")\n\n\n\n\n\n\n\n\n\nHere we see the actual count of the distribution of loans by age. This is essentially showing the same info as the plot above, but giving a number to the loan counts. We see that the distribution of loans follows the distribution of predicting a default, which intuitively makes sense. This shows that approving a loan is less dependent on age and more dependent on the probability of defaulting. But, since our algorthim depends on the data, age groups’ approval rates are biased by their age, not just their applicant status.\n\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\nTo answer this questionn, I examine how loan approval rates differ based on the purpose of the loan. I grouped the data by loan intent (such as education, medical, or business), calculate both the approval rate and the actual default rate for each category, and display these statistics side by side.\n\n\nCode\n# loan intent\nanalyze_loan_intent = df_test.groupby('loan_intent').agg({\n    'approved': 'mean',\n    'loan_status': 'mean'  # Actual default rate\n}).reset_index()\nanalyze_loan_intent['approval_rate'] = analyze_loan_intent['approved'] * 100\nanalyze_loan_intent['default_rate'] = analyze_loan_intent['loan_status'] * 100\nprint(analyze_loan_intent[['loan_intent', 'approval_rate', 'default_rate']])\n\n\n         loan_intent  approval_rate  default_rate\n0  DEBTCONSOLIDATION      90.265487     28.761062\n1          EDUCATION      92.346939     16.751701\n2    HOMEIMPROVEMENT      96.103896     25.000000\n3            MEDICAL      89.655172     28.424977\n4           PERSONAL      91.182365     22.044088\n5            VENTURE      91.804979     14.626556\n\n\nHere we see the approval and default rates based on loan intent. Looking at the approval rates, medical loans do in fact have the lowest approval rate and the second highest actual rate of default. On the other hand, education has the second highest approval rate and the third highest actual rate of default. Venture loans have a similar approval rate but the lowest default rate.\n\n\nCode\n# Loan intent plot\nplt.bar(analyze_loan_intent['loan_intent'], analyze_loan_intent['approval_rate'], color='lightgreen')\nplt.axhline(y=df_test['approved'].mean() * 100, color='r', linestyle='--', label='Overall')\nplt.title('Approval Rate by Loan Purpose')\nplt.xlabel('Loan Purpose')\nplt.ylabel('Approval Rate (%)')\nplt.xticks(rotation=45)\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe plot above helps visualize the numerical values above. The red line shows the overall approval rate, and we see how different intents compare to that overall standard. To answer the question, different intents do indeed have different approval rates.\n\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nCode\n# 3. Analyze by income level\ndf_test['income_category'] = pd.qcut(df_test['person_income'], \n                                    q=5, \n                                    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\nincome_analysis = df_test.groupby('income_category').agg({\n    'approved': 'mean',\n    'loan_status': 'mean'  # Actual default rate\n}).reset_index()\nincome_analysis['approval_rate'] = income_analysis['approved'] * 100\nincome_analysis['default_rate'] = income_analysis['loan_status'] * 100\nprint(income_analysis[['income_category', 'approval_rate', 'default_rate']])\nprint(f\"Overall approval rate: {df_test['approved'].mean() * 100:.1f}%\")\n\n\n  income_category  approval_rate  default_rate\n0        Very Low      80.625543     43.527368\n1             Low      87.403599     23.736075\n2          Medium      94.469224     18.822480\n3            High      96.163906     14.995641\n4       Very High      99.650655     10.131004\nOverall approval rate: 91.6%\n\n\nI created 5 income brackets to help show which segments of perspective borrowers have access to credit. Clearly wealth plays a role: the very high income bracket almost has a 100% approval rate, and the high bracket is not far behind. Going to the very low bracket, it is much lower (80%). It is important though to look at the actual default rates. While the approval rate for very low income is 20% lower than the very high income bracket, the actual default rate is notably 33% higher. So, the approval rate differs logically, except it is interesting how the gap between approval rates and actual default rates is not entirely linear.\n\n\nCode\nplt.bar(income_analysis['income_category'], income_analysis['approval_rate'], color='salmon')\nplt.axhline(y=df_test['approved'].mean() * 100, color='r', linestyle='--', label='Overall')\nplt.title('Approval Rate by Income Level')\nplt.xlabel('Income Level')\nplt.ylabel('Approval Rate (%)')\nplt.xticks(rotation=45)\nplt.legend()\n\n\n\n\n\n\n\n\n\nA simple plot to display how approval rates differ by income level, compared to the overall approval rate.\n\n\nConcluding Thoughts\nTo goal of this blog post was to explore the impact of a purely profit-driven model on access to credit. So, I made an algorithm that works to make the most money for a bank. Consequently, I used the “best” features for modeling loan status and set a threshold value that offered loans to very few people – only those rated with a very low chance of default. These people, unsurprisingly, tended to have high income, be younger in age, and higher success getting a loan for home improvement than medical use. This is blatantly concerning and really delves into the deeper values of aid and capitalism.\nThe people the the hypothetical bank is offering loans to are likely not in need of the loans as badly as the others. A home improvement project by a higher income individual seems magnitudially less crucial than a medical necessity.\nConsidering that people seeking loans for medical expenses have high rates of default, it may seem financially prudent to restrict their access to credit. However, this raises important ethical questions about whether our financial systems should prioritize profit over human welfare, especially when the need stems from unavoidable health crises rather than discretionary spending. So, in my opinion, I strongly value health over profit and thus the weight that we put on the probability of defualting should be lowered when regarding human welfare to give more loan access for medical needs.\nUltimately, this highlights the necessity to define what fairness and equity means in the context of model building. This sheds light on the ethics of model creating an algorithm by defining weights to features that are likely already inherently affected by systemic biases."
  },
  {
    "objectID": "posts/auditing-bias/auditing-bias.html",
    "href": "posts/auditing-bias/auditing-bias.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "This blog post examines the ethical implications of automated decision systems through the lens of employment prediction. Using the American Community Survey (ACS) dataset from New York State, I develop a decision tree classifier to predict employment status based on demographic features while excluding race as a predictor. I then conduct a comprehensive bias audit to evaluate the model’s fairness across racial groups\nMy analysis reveals some disparities (although small) in model performance. I achieve an overall accuracy of 82.4%. I found only a small difference in the accuracy between white and black individuals, with white individuals having an accuracy of 0.829 and black individuals having an accuracy of 0.819.\nThe model does also exhibit some varying false positive and negative rates across racial groups. White individuals experience a false positive rate of 20.0% compared to 20.9% for Black individuals, highlighting subtle differences in error patterns.\nI further investigate three key fairness criteria: calibration, error rate balance, and statistical parity. My findings demonstrate the fundamental impossibility result described by Chouldechova (2017) - when prevalence rates differ between groups, it is mathematically impossible to simultaneously satisfy all fairness criteria. Through visualization of feasible (FNR, FPR) combinations, I illustrate the inherent trade-offs in fairness measures and quantify the adjustments needed to equalize error rates.\nThis work underscores the critical importance of auditing automated decision systems for bias and the necessity of making deliberate, context-specific choices about which fairness criteria to prioritize in real-world applications.\n\n\nCode\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nimport pandas as pd\n\nSTATE = \"NY\"\n\n# data_source = ACSDataSource(survey_year='2018', \n#                             horizon='1-Year', \n#                             survey='person')\n\n# acs_data = data_source.get_data(states=[STATE], download=False)\n\nacs_data = pd.read_csv('/Users/dean@middlebury.edu/Desktop/cs451/andrewdean1.github.io/posts/auditing-bias/psam_p36.csv')\n\nacs_data.head()\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000012\n2\n1\n3802\n1\n36\n1013097\n145\n26\n...\n146\n146\n21\n24\n266\n263\n21\n146\n265\n144\n\n\n1\nP\n2018GQ0000040\n2\n1\n2702\n1\n36\n1013097\n43\n21\n...\n6\n42\n43\n7\n40\n6\n43\n40\n42\n6\n\n\n2\nP\n2018GQ0000060\n2\n1\n2001\n1\n36\n1013097\n88\n18\n...\n88\n163\n161\n162\n87\n12\n162\n88\n87\n88\n\n\n3\nP\n2018GQ0000081\n2\n1\n2401\n1\n36\n1013097\n109\n85\n...\n17\n15\n111\n107\n17\n196\n109\n200\n198\n111\n\n\n4\nP\n2018GQ0000103\n2\n1\n1400\n1\n36\n1013097\n83\n19\n...\n81\n12\n80\n154\n12\n80\n12\n83\n152\n154\n\n\n\n\n5 rows × 286 columns\n\n\n\n\n\nCode\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\n\n\n\n\n\nCode\n# features to use\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n# create the problem: predict employment status ESR, using the race RAC1P as the group label\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n# split into train/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=42)\n\n\n\n\nCode\n# Convert data to DataFrame for easier analysis\nimport pandas as pd\n\n# Create DataFrame from training data\ndf_train = pd.DataFrame(X_train, columns=features_to_use)\ndf_train[\"group\"] = group_train\ndf_train[\"label\"] = y_train\n\n# Create DataFrame from test data\ndf_test = pd.DataFrame(X_test, columns=features_to_use)\ndf_test[\"group\"] = group_test\ndf_test[\"label\"] = y_test\n\n# Combine train and test for overall analysis\ndf_full = pd.concat([df_train, df_test])"
  },
  {
    "objectID": "posts/auditing-bias/auditing-bias.html#abstract",
    "href": "posts/auditing-bias/auditing-bias.html#abstract",
    "title": "Auditing Bias",
    "section": "",
    "text": "This blog post examines the ethical implications of automated decision systems through the lens of employment prediction. Using the American Community Survey (ACS) dataset from New York State, I develop a decision tree classifier to predict employment status based on demographic features while excluding race as a predictor. I then conduct a comprehensive bias audit to evaluate the model’s fairness across racial groups\nMy analysis reveals some disparities (although small) in model performance. I achieve an overall accuracy of 82.4%. I found only a small difference in the accuracy between white and black individuals, with white individuals having an accuracy of 0.829 and black individuals having an accuracy of 0.819.\nThe model does also exhibit some varying false positive and negative rates across racial groups. White individuals experience a false positive rate of 20.0% compared to 20.9% for Black individuals, highlighting subtle differences in error patterns.\nI further investigate three key fairness criteria: calibration, error rate balance, and statistical parity. My findings demonstrate the fundamental impossibility result described by Chouldechova (2017) - when prevalence rates differ between groups, it is mathematically impossible to simultaneously satisfy all fairness criteria. Through visualization of feasible (FNR, FPR) combinations, I illustrate the inherent trade-offs in fairness measures and quantify the adjustments needed to equalize error rates.\nThis work underscores the critical importance of auditing automated decision systems for bias and the necessity of making deliberate, context-specific choices about which fairness criteria to prioritize in real-world applications.\n\n\nCode\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nimport pandas as pd\n\nSTATE = \"NY\"\n\n# data_source = ACSDataSource(survey_year='2018', \n#                             horizon='1-Year', \n#                             survey='person')\n\n# acs_data = data_source.get_data(states=[STATE], download=False)\n\nacs_data = pd.read_csv('/Users/dean@middlebury.edu/Desktop/cs451/andrewdean1.github.io/posts/auditing-bias/psam_p36.csv')\n\nacs_data.head()\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000012\n2\n1\n3802\n1\n36\n1013097\n145\n26\n...\n146\n146\n21\n24\n266\n263\n21\n146\n265\n144\n\n\n1\nP\n2018GQ0000040\n2\n1\n2702\n1\n36\n1013097\n43\n21\n...\n6\n42\n43\n7\n40\n6\n43\n40\n42\n6\n\n\n2\nP\n2018GQ0000060\n2\n1\n2001\n1\n36\n1013097\n88\n18\n...\n88\n163\n161\n162\n87\n12\n162\n88\n87\n88\n\n\n3\nP\n2018GQ0000081\n2\n1\n2401\n1\n36\n1013097\n109\n85\n...\n17\n15\n111\n107\n17\n196\n109\n200\n198\n111\n\n\n4\nP\n2018GQ0000103\n2\n1\n1400\n1\n36\n1013097\n83\n19\n...\n81\n12\n80\n154\n12\n80\n12\n83\n152\n154\n\n\n\n\n5 rows × 286 columns\n\n\n\n\n\nCode\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n\n\n\n\n\n\nCode\n# features to use\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n# create the problem: predict employment status ESR, using the race RAC1P as the group label\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n# split into train/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=42)\n\n\n\n\nCode\n# Convert data to DataFrame for easier analysis\nimport pandas as pd\n\n# Create DataFrame from training data\ndf_train = pd.DataFrame(X_train, columns=features_to_use)\ndf_train[\"group\"] = group_train\ndf_train[\"label\"] = y_train\n\n# Create DataFrame from test data\ndf_test = pd.DataFrame(X_test, columns=features_to_use)\ndf_test[\"group\"] = group_test\ndf_test[\"label\"] = y_test\n\n# Combine train and test for overall analysis\ndf_full = pd.concat([df_train, df_test])"
  },
  {
    "objectID": "posts/auditing-bias/auditing-bias.html#basic-descriptives",
    "href": "posts/auditing-bias/auditing-bias.html#basic-descriptives",
    "title": "Auditing Bias",
    "section": "Basic Descriptives",
    "text": "Basic Descriptives\nHow many individuals are in the data?\n\n\nCode\ntotal_individuals = len(df_full)\nprint(f\"Total number of individuals in the dataset: {total_individuals}\")\n\n\nTotal number of individuals in the dataset: 196967\n\n\nOf these individuals, what proportion have target label equal to 1? In employment prediction, these would correspond to employed individuals.\n\n\nCode\nemployed_proportion = df_full[\"label\"].mean()\nprint(f\"Proportion of employed individuals: {employed_proportion:.4f} ({employed_proportion*100:.2f}%)\")\n\n\nProportion of employed individuals: 0.4644 (46.44%)\n\n\nOf these individuals, how many are in each of the groups?\n\n\nCode\ngroup_counts = df_full[\"group\"].value_counts().sort_index()\nprint(\"\\nNumber of individuals in each racial group:\")\nprint(group_counts)\n\n\n\nNumber of individuals in each racial group:\ngroup\n1    138474\n2     24024\n3       508\n4         5\n5       244\n6     17030\n7        72\n8     10964\n9      5646\nName: count, dtype: int64\n\n\nThe groups are classified as follows: - White alone (1) - Black or African American alone (2) - American Indian alone (3) - Alaska Native alone (4) - American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races (5) - Asian alone (6) - Native Hawaiian and Other Pacific Islander alone (7) - Some Other Race alone (8) - Two or More Races (9)\nIn each group, what proportion of individuals have target label equal to 1?\n\n\nCode\nemployment_by_group = df_full.groupby(\"group\")[\"label\"].mean().sort_index()\nprint(\"\\nProportion of employed individuals by racial group:\")\nfor group, proportion in employment_by_group.items():\n    print(f\"Group {int(group)}: {proportion:.4f} ({proportion*100:.2f}%)\")\n\n\n\nProportion of employed individuals by racial group:\nGroup 1: 0.4736 (47.36%)\nGroup 2: 0.4212 (42.12%)\nGroup 3: 0.4272 (42.72%)\nGroup 4: 0.6000 (60.00%)\nGroup 5: 0.3443 (34.43%)\nGroup 6: 0.4975 (49.75%)\nGroup 7: 0.3889 (38.89%)\nGroup 8: 0.4443 (44.43%)\nGroup 9: 0.3702 (37.02%)\n\n\nBelow I check for intersectional trends by studying the proportion of positive target labels broken out by race groups and sex.\n\n\nCode\n# add the SEX column from the original data\ndf_full[\"SEX\"] = np.concatenate([X_train[:, features_to_use.index(\"SEX\")], \n                                X_test[:, features_to_use.index(\"SEX\")]])\n\n# visualization of employment rates by race and sex\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Calculate employment rates by race and sex\nemployment_by_race_sex = df_full.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index()\nemployment_by_race_sex[\"group\"] = employment_by_race_sex[\"group\"].astype(int)\nemployment_by_race_sex[\"SEX\"] = employment_by_race_sex[\"SEX\"].map({1: \"Male\", 2: \"Female\"})\n\n# Define group labels\ngroup_labels = {\n    1: \"White\",\n    2: \"Black\",\n    3: \"American Indian\",\n    4: \"Alaska Native\",\n    5: \"Am. Indian & Alaska Native\",\n    6: \"Asian\",\n    7: \"Native Hawaiian & Pacific Isl.\",\n    8: \"Other Race\",\n    9: \"Two or More Races\"\n}\n\n# Convert group numbers to labels\nemployment_by_race_sex['group_label'] = employment_by_race_sex['group'].map(group_labels)\n\nplt.figure(figsize=(12, 6))\nsns.barplot(x=\"group_label\", y=\"label\", hue=\"SEX\", data=employment_by_race_sex)\nplt.title(\"Employment Rate by Race and Sex\")\nplt.xlabel(\"Race\")\nplt.ylabel(\"Employment Rate\")\nplt.ylim(0, 1)\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 1. Employment Rate by Race and Sex. We see that for two-thirds of the groups, the employment rate is higher for males. An interesting outlier: male identifying alaskan natives have a 100% employment rate.\n\nBuilding a model\n\n\nCode\n# Building a simple model\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_train, y_train)\nprint(f\"Training Accuracy without optimization: {train_accuracy:.3f}\")\n\n\nTraining Accuracy without optimization: 0.909\n\n\nI fit a decision tree classifier to the training data and receive an accuracy of 91%. This is before I optimize my model by finding the optimal depth and use cross-validation.\n\n\nModel Optimization\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 10, and this optimized model achieves 82.7% accuracy on the training set.\n\n\nCode\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 21)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\nOptimal Depth: 10\nTraining Accuracy with Optimal Depth: 0.827\n\n\nTesting accuracy for the optimized model:\n\n\nCode\ny_hat = dt_classifier_optimal.predict(X_test)\n(y_hat == y_test).mean()\n\n\n0.8236787328019496\n\n\nLooking at two specific groups:\n\n\nCode\n#The accuracy for white individuals is\nwhite_group_accuracy = (y_hat == y_test)[group_test == 1].mean()\nprint(f\"White Accuracy: {white_group_accuracy:.3f}\")\n\n#The accuracy for Black individuals is\nblack_group_accuracy = (y_hat == y_test)[group_test == 2].mean()\nprint(f\"Black Accuracy: {black_group_accuracy:.3f}\")\n\n\nWhite Accuracy: 0.829\nBlack Accuracy: 0.819\n\n\n\n\nOverall Measures on Test Data\nNow that I have my model, I will begin to delve into the intricacies such as its positive predictive value (PPV), false negative rate (FNR), and false positive rate (FPR). This is crucial to begin to understand how the model fluctuates, as later we will hone in on the disparities in these metrics across racial groups.\n\n\nCode\n# Overall Measures on Test Data\nfrom sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n\n# Get predictions on test data\ny_pred = dt_classifier_optimal.predict(X_test)\n\n# 1. Overall accuracy\noverall_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n\n# 2. Positive Predictive Value (Precision)\nppv = precision_score(y_test, y_pred)\nprint(f\"Positive Predictive Value (PPV): {ppv:.4f} ({ppv*100:.2f}%)\")\n\n# 3. Calculate confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n\n# False Negative Rate (FNR) = FN / (FN + TP)\nfnr = fn / (fn + tp)\nprint(f\"False Negative Rate (FNR): {fnr:.4f} ({fnr*100:.2f}%)\")\n\n# False Positive Rate (FPR) = FP / (FP + TN)\nfpr = fp / (fp + tn)\nprint(f\"False Positive Rate (FPR): {fpr:.4f} ({fpr*100:.2f}%)\")\n\nprint(\"\\nConfusion Matrix Summary:\")\nprint(f\"True Positives (TP): {tp}\")\nprint(f\"False Positives (FP): {fp}\")\nprint(f\"True Negatives (TN): {tn}\")\nprint(f\"False Negatives (FN): {fn}\")\n\n\nOverall Accuracy: 0.8237 (82.37%)\nPositive Predictive Value (PPV): 0.7802 (78.02%)\nFalse Negative Rate (FNR): 0.1327 (13.27%)\nFalse Positive Rate (FPR): 0.2146 (21.46%)\n\nConfusion Matrix Summary:\nTrue Positives (TP): 15978\nFalse Positives (FP): 4501\nTrue Negatives (TN): 16470\nFalse Negatives (FN): 2445\n\n\nAbove I display the overall accuracy, positive predictive value, false negative rate, and false positive rate for the model, summarized by a confusion matrix. Before diving deeper into the intricacies of these values, the ratio of TP to FP (and TN to FN) looks like our model is making some good predictions. There seem to be more FP than FN, though, so it will be interesting to look into why that is.\n\n\nAnalysis of Bias: Auditing for Bias Across Racial Groups\nBuilding an “accurate” model is great, but without understanding the potential biases that exist within it, we can only give it so much trust. To begin to understand the disparities in the model, I will audit the model for bias across racial groups.\n\n\nCode\n# Create a DataFrame for analysis\naudit_df = pd.DataFrame({\n    'true': y_test,\n    'pred': y_pred,\n    'group': group_test\n})\n\n# calculate metrics for a specific group\ndef calculate_group_metrics(group_data):\n    y_true = group_data['true']\n    y_pred = group_data['pred']\n    \n    if len(y_true) == 0:\n        return None\n    \n    acc = accuracy_score(y_true, y_pred)\n    \n    prec = precision_score(y_true, y_pred)\n    \n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n    \n    fnr = fn / (fn + tp) if (fn + tp) &gt; 0 else float('nan')\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else float('nan')\n    \n    return {\n        'count': len(y_true),\n        'accuracy': acc,\n        'ppv': prec,\n        'fnr': fnr,\n        'fpr': fpr,\n        'tp': tp,\n        'fp': fp,\n        'tn': tn,\n        'fn': fn\n    }\n\n# get metrics for each group\nresults = {}\nfor group in sorted(audit_df['group'].unique()):\n    group_data = audit_df[audit_df['group'] == group]\n    results[group] = calculate_group_metrics(group_data)\n\n# Create a summary table\nmetrics_df = pd.DataFrame({\n    group: {\n        'Count': results[group]['count'],\n        'Accuracy': results[group]['accuracy'],\n        'PPV': results[group]['ppv'],\n        'FNR': results[group]['fnr'],\n        'FPR': results[group]['fpr']\n    } for group in results\n}).T\n\n# Add overall results\noverall_metrics = calculate_group_metrics(audit_df)\nmetrics_df.loc['Overall'] = [\n    overall_metrics['count'],\n    overall_metrics['accuracy'],\n    overall_metrics['ppv'],\n    overall_metrics['fnr'],\n    overall_metrics['fpr']\n]\n\n# make percentages\nfor col in ['Accuracy', 'PPV', 'FNR', 'FPR']:\n    metrics_df[col] = metrics_df[col].apply(lambda x: f\"{x:.4f} ({x*100:.2f}%)\")\n\nprint(metrics_df)\n\n# Visualize\nmetrics_to_plot = ['accuracy', 'ppv', 'fnr', 'fpr']\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, metric in enumerate(metrics_to_plot):\n    metric_values = [results[group][metric] for group in sorted(results.keys())]\n    \n    ax = axes[i]\n    ax.bar(range(len(results)), metric_values)\n    ax.set_xticks(range(len(results)))\n    ax.set_xticklabels([group_labels[int(g)] for g in sorted(results.keys())], rotation=45, ha='right')\n    ax.set_title(f\"{metric.upper()} by Racial Group\")\n    ax.axhline(overall_metrics[metric], color='red', linestyle='--', label='Overall')\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n           Count          Accuracy               PPV              FNR  \\\n1        27632.0   0.8286 (82.86%)   0.7972 (79.72%)  0.1402 (14.02%)   \n2         4786.0   0.8191 (81.91%)   0.7548 (75.48%)  0.1431 (14.31%)   \n3          106.0   0.8396 (83.96%)   0.7969 (79.69%)   0.0727 (7.27%)   \n5           35.0   0.8857 (88.57%)   0.8125 (81.25%)   0.0714 (7.14%)   \n6         3471.0   0.7862 (78.62%)   0.7247 (72.47%)   0.0858 (8.58%)   \n7           13.0  1.0000 (100.00%)  1.0000 (100.00%)   0.0000 (0.00%)   \n8         2208.0   0.8175 (81.75%)   0.7398 (73.98%)   0.0930 (9.30%)   \n9         1143.0   0.8443 (84.43%)   0.7447 (74.47%)  0.1422 (14.22%)   \nOverall  39394.0   0.8237 (82.37%)   0.7802 (78.02%)  0.1327 (13.27%)   \n\n                     FPR  \n1        0.1999 (19.99%)  \n2        0.2094 (20.94%)  \n3        0.2549 (25.49%)  \n5        0.1429 (14.29%)  \n6        0.3385 (33.85%)  \n7         0.0000 (0.00%)  \n8        0.2537 (25.37%)  \n9        0.1633 (16.33%)  \nOverall  0.2146 (21.46%)  \n\n\n\n\n\n\n\n\n\nAbove I calculate the overall accuracy, PPV, FNR, and FPR for my model on the test data. I also break down these metrics by racial group to identify potential disparities, showing a summary table showing all metrics for each group and visualizing the disparities across groups with bar charts. We see that the FNR is significantly higher for white, black, and two or more races individuals, meaning that the model is more likely to incorrectly classify employed individuals as unemployed for these groups. Further, the FPR is much higher for asian individuals, meaning that the model is more likely to incorrectly classify unemployed individuals as employed for asian individuals. We see that overall, the false positive rates are higher than the false negative rates, meaning that the model is more likely to incorrectly classify unemployed individuals as employed. Overall, however, the accuracy and PPV by group are pretty fixed around the overall rates."
  },
  {
    "objectID": "posts/auditing-bias/auditing-bias.html#calibration-error-rate-balance-and-statistical-parity",
    "href": "posts/auditing-bias/auditing-bias.html#calibration-error-rate-balance-and-statistical-parity",
    "title": "Auditing Bias",
    "section": "Calibration, Error Rate Balance, and Statistical Parity",
    "text": "Calibration, Error Rate Balance, and Statistical Parity\nNext we will delve into 3 specific measures of bias: calibration, error rate balance, and statistical parity. To quickly summarize the measures: - Calibration: A score is well-calibrated if it reflects the same likelihood of employment irrespective of the individuals’ group membership. Calibration in this context is similar to saying “free from predictive bias.” - Statistical parity: A score S = S(x) satisfies statistical parity if the proportion of individuals classified as employed is the same for each group. Are group-specific false positive rates equal to group-specific false negative rates? - Error rate balance: A score S = S(x) satisfies error rate balance if the false positive and false negative error rates are equal across groups.\nLet’s see how our model performs on these measures. For each measure of bias, I calculate the important measures by group and overall. For the calibration analysis, the PPV by group is crucial: a well-calibrated model should have similar PPV across all groups. For the error rate balance, it is most important to look at the FPR and FNR by group: these should be similar across all groups. For the statistical parity, the prediction rates should be similar across groups.\n\n\nCode\n# calculate all bias measures\ndef calculate_bias_measures(df):\n    groups = sorted(df['group'].unique())\n    results = {}\n    \n    # Overall positive prediction rate\n    overall_ppr = df['pred'].mean()\n    \n    for group in groups:\n        group_data = df[df['group'] == group]\n        \n        # Skip groups with insufficient data\n        if len(group_data) &lt; 10:\n            continue\n            \n        # 1. Calibration: PPV should be similar across groups\n        # this means PPV should be similar across groups\n        pred_positive = group_data[group_data['pred'] == 1]\n        if len(pred_positive) &gt; 0:\n            calibration = pred_positive['true'].mean()  #PPV\n        else:\n            calibration = float('nan')\n        \n        # 3. Statistical Parity: FPR and FNR should be similar across groups\n        stat_parity = group_data['pred'].mean()\n        \n        results[group] = {\n            'count': len(group_data),\n            'calibration': calibration,\n            'stat_parity': stat_parity,\n            'stat_parity_ratio': stat_parity / overall_ppr if overall_ppr &gt; 0 else float('nan')\n        }\n    \n    return results, overall_ppr\n\n# get bias measures\nbias_results, overall_ppr = calculate_bias_measures(audit_df)\n\n# Create summary tables\nprint(\"\\n1. Calibration Analysis (PPV by group)\")\nprint(\"A well-calibrated model should have similar PPV across all groups\")\ncalibration_df = pd.DataFrame({\n    group: {\n        'Count': bias_results[group]['count'],\n        'PPV (Calibration)': bias_results[group]['calibration'],\n        'Overall PPV': overall_metrics['ppv'],\n        'Difference': bias_results[group]['calibration'] - overall_metrics['ppv']\n    } for group in bias_results\n}).T\n\n# Format as percentages\nfor col in ['PPV (Calibration)', 'Overall PPV', 'Difference']:\n    calibration_df[col] = calibration_df[col].apply(lambda x: f\"{x:.4f} ({x*100:.2f}%)\")\n\nprint(calibration_df)\n\nprint(\"\\n2. Error Rate Balance Analysis\")\nprint(\"Error rate balance requires similar FPR and FNR across all groups\")\nerror_balance_df = pd.DataFrame({\n    group: {\n        'Count': bias_results[group]['count'],\n        'FPR': results[group]['fpr'],\n        'Overall FPR': overall_metrics['fpr'],\n        'FPR Difference': results[group]['fpr'] - overall_metrics['fpr'],\n        'FNR': results[group]['fnr'],\n        'Overall FNR': overall_metrics['fnr'],\n        'FNR Difference': results[group]['fnr'] - overall_metrics['fnr']\n    } for group in bias_results\n}).T\n\nfor col in ['FPR', 'Overall FPR', 'FPR Difference', 'FNR', 'Overall FNR', 'FNR Difference']:\n    error_balance_df[col] = error_balance_df[col].apply(lambda x: f\"{x:.4f} ({x*100:.2f}%)\")\n\nprint(error_balance_df)\n\nprint(\"\\n3. Statistical Parity Analysis\")\nprint(\"Statistical parity requires similar prediction rates across all groups\")\nstat_parity_df = pd.DataFrame({\n    group: {\n        'Count': bias_results[group]['count'],\n        'Prediction Rate': bias_results[group]['stat_parity'],\n        'Overall Rate': overall_ppr,\n        'Difference': bias_results[group]['stat_parity'] - overall_ppr,\n        'Ratio': bias_results[group]['stat_parity_ratio']\n    } for group in bias_results\n}).T\n\nfor col in ['Prediction Rate', 'Overall Rate', 'Difference']:\n    stat_parity_df[col] = stat_parity_df[col].apply(lambda x: f\"{x:.4f} ({x*100:.2f}%)\")\n\nprint(stat_parity_df)\n\n# Visualize the three fairness criteria\nfig, axes = plt.subplots(3, 1, figsize=(14, 15))\n\n# 1. Calibration\ncalibration_values = [bias_results[group]['calibration'] for group in sorted(bias_results.keys())]\naxes[0].bar(range(len(bias_results)), calibration_values)\naxes[0].set_xticks(range(len(bias_results)))\naxes[0].set_xticklabels([group_labels[int(g)] for g in sorted(bias_results.keys())], rotation=45, ha='right')\naxes[0].set_title(\"Calibration (PPV) by Racial Group\")\naxes[0].axhline(overall_metrics['ppv'], color='red', linestyle='--', label='Overall PPV')\naxes[0].set_ylim(0, 1)\naxes[0].legend()\n\n# 2. Error Rate Balance\nx = np.arange(len(bias_results))\nwidth = 0.35\nfpr_values = [results[group]['fpr'] for group in sorted(bias_results.keys())]\nfnr_values = [results[group]['fnr'] for group in sorted(bias_results.keys())]\n\naxes[1].bar(x - width/2, fpr_values, width, label='FPR')\naxes[1].bar(x + width/2, fnr_values, width, label='FNR')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels([group_labels[int(g)] for g in sorted(bias_results.keys())], rotation=45, ha='right')\naxes[1].set_title(\"Error Rates by Racial Group\")\naxes[1].axhline(overall_metrics['fpr'], color='blue', linestyle='--', label='Overall FPR')\naxes[1].axhline(overall_metrics['fnr'], color='orange', linestyle='--', label='Overall FNR')\naxes[1].set_ylim(0, max(max(fpr_values), max(fnr_values)) * 1.2)\naxes[1].legend()\n\n# 3. Statistical Parity\nstat_parity_values = [bias_results[group]['stat_parity'] for group in sorted(bias_results.keys())]\naxes[2].bar(range(len(bias_results)), stat_parity_values)\naxes[2].set_xticks(range(len(bias_results)))\naxes[2].set_xticklabels([group_labels[int(g)] for g in sorted(bias_results.keys())], rotation=45, ha='right')\naxes[2].set_title(\"Statistical Parity (Prediction Rate) by Racial Group\")\naxes[2].axhline(overall_ppr, color='red', linestyle='--', label='Overall Prediction Rate')\naxes[2].set_ylim(0, 1)\naxes[2].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Summary\nprint(\"\\n--- Summary of Fairness Criteria ---\")\nprint(\"1. Calibration: Is PPV similar across groups?\")\ncalibration_diffs = [abs(bias_results[group]['calibration'] - overall_metrics['ppv']) for group in bias_results]\nmax_calibration_diff = max(calibration_diffs)\nif max_calibration_diff &lt; 0.05:\n    print(\"✓ Model appears to be well-calibrated (max PPV difference &lt; 5%)\")\nelse:\n    print(\"✗ Model shows calibration disparities (max PPV difference: {:.2f}%)\".format(max_calibration_diff*100))\n\nprint(\"\\n2. Error Rate Balance: Are FPR and FNR similar across groups?\")\nfpr_diffs = [abs(results[group]['fpr'] - overall_metrics['fpr']) for group in bias_results]\nfnr_diffs = [abs(results[group]['fnr'] - overall_metrics['fnr']) for group in bias_results]\nmax_fpr_diff = max(fpr_diffs)\nmax_fnr_diff = max(fnr_diffs)\nif max_fpr_diff &lt; 0.05 and max_fnr_diff &lt; 0.05:\n    print(\"✓ Model satisfies approximate error rate balance (max differences &lt; 5%)\")\nelse:\n    print(\"✗ Model shows error rate disparities:\")\n    print(\"  - Max FPR difference: {:.2f}%\".format(max_fpr_diff*100))\n    print(\"  - Max FNR difference: {:.2f}%\".format(max_fnr_diff*100))\n\nprint(\"\\n3. Statistical Parity: Is prediction rate similar across groups?\")\nstat_parity_diffs = [abs(bias_results[group]['stat_parity'] - overall_ppr) for group in bias_results]\nmax_stat_parity_diff = max(stat_parity_diffs)\nif max_stat_parity_diff &lt; 0.05:\n    print(\"✓ Model satisfies approximate statistical parity (max difference &lt; 5%)\")\nelse:\n    print(\"✗ Model shows statistical parity disparities (max difference: {:.2f}%)\".format(max_stat_parity_diff*100))\n\n\n\n1. Calibration Analysis (PPV by group)\nA well-calibrated model should have similar PPV across all groups\n     Count PPV (Calibration)      Overall PPV        Difference\n1  27632.0   0.7972 (79.72%)  0.7802 (78.02%)    0.0170 (1.70%)\n2   4786.0   0.7548 (75.48%)  0.7802 (78.02%)  -0.0254 (-2.54%)\n3    106.0   0.7969 (79.69%)  0.7802 (78.02%)    0.0167 (1.67%)\n5     35.0   0.8125 (81.25%)  0.7802 (78.02%)    0.0323 (3.23%)\n6   3471.0   0.7247 (72.47%)  0.7802 (78.02%)  -0.0555 (-5.55%)\n7     13.0  1.0000 (100.00%)  0.7802 (78.02%)   0.2198 (21.98%)\n8   2208.0   0.7398 (73.98%)  0.7802 (78.02%)  -0.0404 (-4.04%)\n9   1143.0   0.7447 (74.47%)  0.7802 (78.02%)  -0.0355 (-3.55%)\n\n2. Error Rate Balance Analysis\nError rate balance requires similar FPR and FNR across all groups\n     Count              FPR      Overall FPR     FPR Difference  \\\n1  27632.0  0.1999 (19.99%)  0.2146 (21.46%)   -0.0147 (-1.47%)   \n2   4786.0  0.2094 (20.94%)  0.2146 (21.46%)   -0.0052 (-0.52%)   \n3    106.0  0.2549 (25.49%)  0.2146 (21.46%)     0.0403 (4.03%)   \n5     35.0  0.1429 (14.29%)  0.2146 (21.46%)   -0.0718 (-7.18%)   \n6   3471.0  0.3385 (33.85%)  0.2146 (21.46%)    0.1238 (12.38%)   \n7     13.0   0.0000 (0.00%)  0.2146 (21.46%)  -0.2146 (-21.46%)   \n8   2208.0  0.2537 (25.37%)  0.2146 (21.46%)     0.0390 (3.90%)   \n9   1143.0  0.1633 (16.33%)  0.2146 (21.46%)   -0.0514 (-5.14%)   \n\n               FNR      Overall FNR     FNR Difference  \n1  0.1402 (14.02%)  0.1327 (13.27%)     0.0075 (0.75%)  \n2  0.1431 (14.31%)  0.1327 (13.27%)     0.0104 (1.04%)  \n3   0.0727 (7.27%)  0.1327 (13.27%)   -0.0600 (-6.00%)  \n5   0.0714 (7.14%)  0.1327 (13.27%)   -0.0613 (-6.13%)  \n6   0.0858 (8.58%)  0.1327 (13.27%)   -0.0469 (-4.69%)  \n7   0.0000 (0.00%)  0.1327 (13.27%)  -0.1327 (-13.27%)  \n8   0.0930 (9.30%)  0.1327 (13.27%)   -0.0397 (-3.97%)  \n9  0.1422 (14.22%)  0.1327 (13.27%)     0.0094 (0.94%)  \n\n3. Statistical Parity Analysis\nStatistical parity requires similar prediction rates across all groups\n     Count  Prediction Rate     Overall Rate         Difference     Ratio\n1  27632.0  0.5150 (51.50%)  0.5199 (51.99%)   -0.0048 (-0.48%)  0.990705\n2   4786.0  0.4875 (48.75%)  0.5199 (51.99%)   -0.0324 (-3.24%)  0.937699\n3    106.0  0.6038 (60.38%)  0.5199 (51.99%)     0.0839 (8.39%)  1.161436\n5     35.0  0.4571 (45.71%)  0.5199 (51.99%)   -0.0627 (-6.27%)  0.879373\n6   3471.0  0.6226 (62.26%)  0.5199 (51.99%)    0.1027 (10.27%)  1.197627\n7     13.0  0.3846 (38.46%)  0.5199 (51.99%)  -0.1352 (-13.52%)  0.739857\n8   2208.0  0.5430 (54.30%)  0.5199 (51.99%)     0.0232 (2.32%)  1.044579\n9   1143.0  0.4112 (41.12%)  0.5199 (51.99%)  -0.1087 (-10.87%)  0.790994\n\n\n\n\n\n\n\n\n\n\n--- Summary of Fairness Criteria ---\n1. Calibration: Is PPV similar across groups?\n✗ Model shows calibration disparities (max PPV difference: 21.98%)\n\n2. Error Rate Balance: Are FPR and FNR similar across groups?\n✗ Model shows error rate disparities:\n  - Max FPR difference: 21.46%\n  - Max FNR difference: 13.27%\n\n3. Statistical Parity: Is prediction rate similar across groups?\n✗ Model shows statistical parity disparities (max difference: 13.52%)\n\n\nAs shown by the summary of the fairness criteria, my model does not satisfy all three criteria simultaneously. I calculate the max difference for each criterion (the difference between the group and overall) and found that the model shows disparities in all 3 aspects. The max FPR and PPV difference were the largest, 21.46% and 21.98% respectively.\nThe calibration disparity of nearly 22% indicates that when my model predicts employment for certain racial groups, its confidence is significantly misaligned with actual outcomes. Similarly, the error rate imbalances reveal that the model’s mistakes are not distributed equitably, with some groups experiencing false positives at rates over 21 percentage points higher than others. The statistical parity disparity of 13.52% further demonstrates that the model’s predictions themselves are not demographically balanced.\n\nRecreating Figure 5 from Chouldechova (2017)\n\n\nCode\n# Create a figure showing feasible (FNR, FPR) combinations\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\n# Calculates feasible FPR given FNR, prevalence, and PPV\ndef calculate_feasible_fpr(fnr, prevalence, ppv):\n    # Based on the relationship: PPV = (prevalence * (1-FNR)) / (prevalence * (1-FNR) + (1-prevalence) * FPR)\n    # Solving for FPR: FPR = (prevalence * (1-FNR) * (1-PPV)) / ((1-prevalence) * PPV)\n    if ppv == 1:  # Handle edge case\n        return 0\n    return (prevalence * (1-fnr) * (1-ppv)) / ((1-prevalence) * ppv)\n\n# group 1 is White group and group 2 is Black group\nwhite_group_data = audit_df[audit_df['group'] == 1]\nblack_group_data = audit_df[audit_df['group'] == 2]\n\n# Calculate prevalence for each group\nprevalence_white = white_group_data['true'].mean()\nprevalence_black = black_group_data['true'].mean()\n\n# Calculate observed FNR, FPR, and PPV for each group\nwhite_metrics = calculate_group_metrics(white_group_data)\nblack_metrics = calculate_group_metrics(black_group_data)\n\nfnr_white = white_metrics['fnr']\nfpr_white = white_metrics['fpr']\nppv_white = white_metrics['ppv']\n\nfnr_black = black_metrics['fnr']\nfpr_black = black_metrics['fpr']\nppv_black = black_metrics['ppv']\n\n# Create the figure\nplt.figure(figsize=(10, 8))\n\n# Plot the observed points\nplt.scatter(fnr_white, fpr_white, color='blue', s=100, label=f'White (Group 1): FNR={fnr_white:.3f}, FPR={fpr_white:.3f}')\nplt.scatter(fnr_black, fpr_black, color='red', s=100, label=f'Black (Group 2): FNR={fnr_black:.3f}, FPR={fpr_black:.3f}')\n\n# Generate feasible (FNR, FPR) combinations for White individuals\nfnr_range = np.linspace(0, 1, 1000)\nfpr_white_feasible = [calculate_feasible_fpr(fnr, prevalence_white, ppv_white) for fnr in fnr_range]\n\n# Plot the feasible line for White individuals\nplt.plot(fnr_range, fpr_white_feasible, color='orange', linewidth=2, \n         label=f'Feasible (FNR, FPR) for White, PPV={ppv_white:.3f}')\n\n# Generate feasible (FNR, FPR) combinations for Black individuals with PPV equal to White PPV\nfpr_black_feasible_equal_ppv = [calculate_feasible_fpr(fnr, prevalence_black, ppv_white) for fnr in fnr_range]\n\n# Plot the feasible line for Black individuals with PPV equal to White PPV\nplt.plot(fnr_range, fpr_black_feasible_equal_ppv, color='darkgrey', linewidth=2,\n         label=f'Feasible (FNR, FPR) for Black, PPV={ppv_white:.3f}')\n\n# Generate and plot feasible regions with varying PPV constraints based on Chouldechova (2017)\ndelta_values = [0.05, 0.1, 0.125]\ncolors = ['#4682B4', '#ADD8E6', '#89CFF0']\n\nfor i, delta in enumerate(delta_values):\n    # Lower bound of PPV\n    ppv_lower = max(0, ppv_white - delta)\n    fpr_black_lower = [calculate_feasible_fpr(fnr, prevalence_black, ppv_lower) for fnr in fnr_range]\n    \n    # Upper bound of PPV\n    ppv_upper = min(1, ppv_white + delta)\n    fpr_black_upper = [calculate_feasible_fpr(fnr, prevalence_black, ppv_upper) for fnr in fnr_range]\n    \n    # Create polygon vertices\n    vertices = [(0, 0)]  # Start at origin\n    for j in range(len(fnr_range)):\n        if 0 &lt;= fpr_black_lower[j] &lt;= 1:\n            vertices.append((fnr_range[j], fpr_black_lower[j]))\n    \n    # Add upper bound in reverse lol\n    for j in range(len(fnr_range)-1, -1, -1):\n        if 0 &lt;= fpr_black_upper[j] &lt;= 1:\n            vertices.append((fnr_range[j], fpr_black_upper[j]))\n    \n    vertices.append((0, 0))  # Close it\n    \n    # Create and add\n    polygon = Polygon(vertices, closed=True, alpha=0.3, color=colors[i],\n                     label=f'Feasible region for |PPV - {ppv_white:.3f}| &lt; {delta}')\n    plt.gca().add_patch(polygon)\n\nplt.xlabel('False Negative Rate (FNR)')\nplt.ylabel('False Positive Rate (FPR)')\nplt.title('Feasible (FNR, FPR) Combinations by Group')\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.grid(True, alpha=0.3)\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\nplt.tight_layout()\nplt.show()\n\n# Print key metrics for reference\nprint(f\"White (Group 1): Prevalence={prevalence_white:.3f}, PPV={ppv_white:.3f}, FNR={fnr_white:.3f}, FPR={fpr_white:.3f}\")\nprint(f\"Black (Group 2): Prevalence={prevalence_black:.3f}, PPV={ppv_black:.3f}, FNR={fnr_black:.3f}, FPR={fpr_black:.3f}\")\n\n\n\n\n\n\n\n\n\nWhite (Group 1): Prevalence=0.478, PPV=0.797, FNR=0.140, FPR=0.200\nBlack (Group 2): Prevalence=0.429, PPV=0.755, FNR=0.143, FPR=0.209\n\n\n\n\nFigure 5 from Chouldechova (2017).\nPlots the observed (FNR, FPR) points for White (Group 1) and Black (Group 2) individuals. The orange line represents the feasible (FNR, FPR) combinations for White individuals with fixed prevalence and PPV. The dark grey line represents the feasible (FNR, FPR) combinations for Black individuals when PPV is set equal to the White PPV. Similarly to Chouldechova, I create nested shaded regions showing feasible (FNR, FPR) combinations for Black individuals when PPV is allowed to vary within different thresholds (δ = 0.05, 0.1, 0.125). The visualization illustrates the impossibility theorem described by Chouldechova - that when prevalence rates differ between groups, it’s mathematically impossible to simultaneously achieve calibration, error rate balance, and statistical parity unless the classifier is perfect. This figure also demonstrates the inherent trade-offs in fairness criteria and explains why achieving all fairness definitions simultaneously is often impossible in real-world scenarios.\nIf we desired to tune our classifier threshold so that the false positive rates were equal between groups, how much would we need to change the false negative rate?\nTo look into this, we need to examine the feasible (FNR, FPR) combinations shown in our plot, particularly focusing on the dark grey line that represents feasible combinations for Black individuals when their PPV equals the White PPV. Equation 2.6 states the following: \\[\\text{FPR} = \\frac{p}{1-p} \\cdot \\frac{1-\\text{PPV}}{\\text{PPV}} \\cdot (1-\\text{FNR})\\]\nChelnova discusses 3 possible tuning strategies when prevelence differs between groups. The relevant one for us is to allow unequal FNRs to maintain equal PPVs and achieve equal FPRs. Looking at our data above and following this strategy, we can: - Set the target FPR for Black individuals equal to White FPR (0.200) - Set the PPV for Black individuals equal to White PPV (0.797) - Solve for the required FNR for Black individuals\nRearranging equation 2.6 to solve for FNR, we get: \\[\\text{FNR} = 1 - \\frac{\\text{FPR} \\cdot (1-p) \\cdot \\text{PPV}}{p \\cdot (1-\\text{PPV})}\\]\nAnd so solve for the FNR with White PPV: \\[\\text{FNR} = 1 - \\frac{0.200 \\cdot (1-0.429) \\cdot 0.797}{0.429 \\cdot (1-0.797)} = -0.45\\]\nThis negative value indicates that it’s mathematically impossible to achieve equal FPRs while maintaining equal PPVs with our current prevalence rates. The equation is telling us we would need a negative FNR, which is not feasible. This aligns with Chouldechova’s discussion - when prevalence rates differ significantly, enforcing equal FPRs while maintaining calibration (equal PPVs) can require extreme and often impossible adjustments to the FNR.\nIf we instead relax the equal PPV constraint and use the actual Black PPV (0.755), we can achieve equal FPRs while maintaining the current PPVs: \\[\\text{FNR} = 1 - \\frac{0.200 \\cdot (1-0.429) \\cdot 0.755}{0.429 \\cdot (1-0.755)} = 0.180\\]\nThis means we would need to increase the FNR for Black individuals from 0.143 to about 0.180 to achieve equal FPRs while maintaining the current PPVs. This demonstrates the fundamental trade-off highlighted in Chouldechova’s paper: when prevalence differs between groups, we cannot simultaneously achieve equal error rates and equal predictive values."
  },
  {
    "objectID": "posts/auditing-bias/auditing-bias.html#concluding-discussion",
    "href": "posts/auditing-bias/auditing-bias.html#concluding-discussion",
    "title": "Auditing Bias",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial or governmental use?\nMy model could be valuable to companies in both private and public sectors. Recruitment agencies and HR departments might leverage such a system to identify promising candidates and streamline hiring processes. Insurance companies could use similar models to assess risk profiles when offering unemployment insurance. Financial institutions might incorporate employment prediction into their lending algorithms to evaluate loan applicants’ ability to repay (relating to my past blog post which you can find here).\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\nThe deployment of my model in real-world settings could have some far-reaching implications. Our bias audit revealed subtle but still meaningful disparities in error rates across racial groups. While the differences may appear small (e.g., a 0.9 percentage point difference in false positive rates between White and Black individuals), when applied to millions of people, these disparities could affect thousands of individuals unfairly.\nMore fundamentally, my analysis of fairness criteria demonstrates the difficulty of simultaneously satisfying multiple definitions of fairness when base rates differ between groups. This presents an ethical dilemma for any organization deploying such systems: which fairness criterion should be prioritized? Should we ensure equal false positive rates to prevent unfair denials of opportunity? Or should we focus on equal false negative rates to ensure no group is disproportionately overlooked?\nBased on your bias audit, do you feel that your model displays problematic bias? What kinds (calibration, error rate, etc)?\nBased on my bias audit, I believe my model does exhibit problematic bias, though the disparities are pretty subtle. The most concerning bias appears in the error rate balance across racial groups. My analysis showed a difference in false positive rates between White individuals (20.0%) and Black individuals (20.9%). While this 0.9 percentage point difference might seem small, it represents a systematic disadvantage that would affect thousands of people if deployed at scale.\nWhen I attempted to mathematically equalize these error rates using Chouldechova’s equation 2.6, I discovered that achieving equal FPRs while maintaining equal PPVs would require an impossible negative FNR value. Even when relaxing the equal PPV constraint, equalizing FPRs would require increasing the FNR for Black individuals from 14% to 18% - a significant adjustment that could result in more disparities.\nThe model also shows some calibration differences, with PPV values of 79.7% for White individuals versus 75.5% for Black individuals. This 4.2 percentage point difference means that when my model predicts employment for Black individuals, it’s less likely to be correct than when it makes the same prediction for White individuals.\nMy visualization of feasible (FNR, FPR) combinations clearly illustrates these trade-offs, showing that the observed disparities are not merely implementation issues but fundamental mathematical constraints given the different prevalence rates between groups (47.8% for White individuals versus 42.9% for Black individuals).\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?\nYes, there are some additional aspects beyond bias that make me uncomfortable. First, my model relies heavily on demographic features like age, education, and marital status. While these features have predictive power, they raise questions about reinforcing existing social patterns rather than evaluating individual merit. For example, my decision classifier likely penalizes younger individuals with less education, potentially creating a self-reinforcing cycle where those already disadvantaged in the job market face additional algorithmic barriers. Second, my model’s overall accuracy of 82.4%, while not terrible, still means about one in five predictions is incorrect. For important decisions affecting people’s livelihoods, this error rate is concerning, especially when errors disproportionately affect certain groups. Lastly, my model lacks transparency for the individuals being evaluated. Someone denied an opportunity based on my prediction would have little insight into why they received an unfavorable assessment or what they could do to improve their prospects.\nThere are some ways to address these concerns. To start, we could incorprate some human judgment: Deploy the model as a decision support tool rather than an autonomous decision-maker, ensuring human oversight for all consequential decisions. Next, we could continuously monitor and adjust the model by implementing ongoing bias audits across multiple dimensions and adjust the model as needed. A last thought that I have is to potentially introduce stakeholders to the model. This could look like including representatives from potentially adversly affected communities in the design, deployment, and governance of the model."
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html",
    "href": "posts/perceptron-blog/perceptron-blog.html",
    "title": "Perceptron",
    "section": "",
    "text": "In this blog post, I dive into implementing the perceptron and run several experiments to explore its capabilities and limitations. The perceptron is designed to predict binary outcomes from data represented in a continuous, finite-dimensional feature space.\nMy experiments demonstrate that the perceptron successfully converges to a solution that perfectly separates classes when the data is linearly separable. However, it fails to converge when the data is not linearly separable.\nI then extend this approach by implementing a more generalized version called the minibatch perceptron. This variation processes k observations per iteration instead of just one. I show that when k=1, the minibatch perceptron behaves identically to the standard perceptron. As k increases toward n (the total number of data points), the algorithm continues to find a decision boundary on linearly separable data. Interestingly, when k=n, the minibatch perceptron can converge on non-linearly separable data — though the resulting boundary won’t classify every point correctly.\nFor the full implementation of both algorithms, check out perceptron.py and minibatch_perceptron.py.\nAs shown by Prof Chodrow, the perceptron algorithm follows an algorithmic process as follows: Randomly select an initial decision boundary \\(w\\), then repeat the following process until convergence: 1. Pick a random intenger \\(i\\) 2. Compute the score of the point \\(x_i\\) 3. Update the decision boundary \\(w\\)\nIn perceptron.py, I implement this process. I will only highlight my gradient function, as it turns the crucial math of the perceptron algo into code. My perceptron.grad() function is as follows:\ndef grad(self, X, y):\n    s = X@self.w\n    return (s*y &lt; 0)*X*y\nFirstly, I take an input \\(X_i\\) and calculate the dot product against the weight value \\(w\\). Next, I use that result to calculate \\([s_iy_i&lt;0]y_iX_i\\)."
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html#abstract",
    "href": "posts/perceptron-blog/perceptron-blog.html#abstract",
    "title": "Perceptron",
    "section": "",
    "text": "In this blog post, I dive into implementing the perceptron and run several experiments to explore its capabilities and limitations. The perceptron is designed to predict binary outcomes from data represented in a continuous, finite-dimensional feature space.\nMy experiments demonstrate that the perceptron successfully converges to a solution that perfectly separates classes when the data is linearly separable. However, it fails to converge when the data is not linearly separable.\nI then extend this approach by implementing a more generalized version called the minibatch perceptron. This variation processes k observations per iteration instead of just one. I show that when k=1, the minibatch perceptron behaves identically to the standard perceptron. As k increases toward n (the total number of data points), the algorithm continues to find a decision boundary on linearly separable data. Interestingly, when k=n, the minibatch perceptron can converge on non-linearly separable data — though the resulting boundary won’t classify every point correctly.\nFor the full implementation of both algorithms, check out perceptron.py and minibatch_perceptron.py.\nAs shown by Prof Chodrow, the perceptron algorithm follows an algorithmic process as follows: Randomly select an initial decision boundary \\(w\\), then repeat the following process until convergence: 1. Pick a random intenger \\(i\\) 2. Compute the score of the point \\(x_i\\) 3. Update the decision boundary \\(w\\)\nIn perceptron.py, I implement this process. I will only highlight my gradient function, as it turns the crucial math of the perceptron algo into code. My perceptron.grad() function is as follows:\ndef grad(self, X, y):\n    s = X@self.w\n    return (s*y &lt; 0)*X*y\nFirstly, I take an input \\(X_i\\) and calculate the dot product against the weight value \\(w\\). Next, I use that result to calculate \\([s_iy_i&lt;0]y_iX_i\\)."
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html#part-a-implementing-perceptron",
    "href": "posts/perceptron-blog/perceptron-blog.html#part-a-implementing-perceptron",
    "title": "Perceptron",
    "section": "Part A: Implementing Perceptron",
    "text": "Part A: Implementing Perceptron\n\n\nCode\n%load_ext autoreload\n%autoreload 2\nimport torch\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom perceptron import Perceptron, PerceptronOptimizer\nfrom perceptron_minibatch import MinibatchPerceptron, MinibatchPerceptronOptimizer\n\n\n\n\nCode\n%reload_ext autoreload\n\n\nTo check my perceptron implementation, I first create a set of data that is clearly linearly separable.\n\n\nCode\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    y = 2*y - 1\n\n    return X, y\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\ntorch.manual_seed(42)\n\n# create linearly separable data\nX_ls, y_ls = perceptron_data(n_points = 50, noise = 0.3)\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X_ls, y_ls, ax)\nax.set_title(\"Our Linearly Separable Data\");\n\n\n\n\n\n\n\n\n\nNext, I run my perceptron algo on this data, printing the loss at each iteration. Because our minimal training loop terminates, we know we converge to 0.\n\n\nCode\ntorch.manual_seed(42)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\nloss_vec = []\n\nwhile loss &gt; 0: \n    loss = p.loss(X_ls, y_ls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_ls, y_ls)"
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html#part-b-fitting-and-evaluating-perceptron-models",
    "href": "posts/perceptron-blog/perceptron-blog.html#part-b-fitting-and-evaluating-perceptron-models",
    "title": "Perceptron",
    "section": "Part B: Fitting and Evaluating Perceptron Models",
    "text": "Part B: Fitting and Evaluating Perceptron Models\n\nPart B.1: The Perceptron Algorithm on Linearly Separable Data\nFrom our data above, I illustrate how the loss function changes between iterations of the minimal training loop.\n\n\nCode\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\")\n\n\nText(0.5, 1.0, 'Loss Function Throughout Model Training')\n\n\n\n\n\n\n\n\n\nLooking at how the loss changes with respect to the perceptron iteration, we can see that the loss is decreasing as the perceptron updates. This is expected, as the perceptron is designed to find a decision boundary that perfectly separates the data. The loss begins at 0.4, and eventually converges to 0 through almost 60 iterations. Through these iterations, though, the weight vector only changes 6 times. In all other iterations, the randomly selected point was correctly classified by the model at that stage.\nTo visualize how the decision boundary evolves during training, the figure below tracks changes to the weight vector. Each subplot shows a weight vector update, with dashed lines representing the previous boundary and solid lines showing the new one. The circled point in each subplot indicates the misclassified example that triggered the update.\n\n\nCode\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\ntorch.manual_seed(1234567)\n\np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X_ls, y_ls)\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nfig, axarr = plt.subplots(3, 4, sharex=True, sharey=True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1: 0, 1: 1}\n\ncurrent_ax = 0\nloss = 1\nloss_vec = []\nmax_plots = axarr.size\n\nupdates = []\n\nwhile loss &gt; 0:\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step\n    i, local_loss = opt.step(X_ls, y_ls)\n\n    # if a change was made, record the update\n    if local_loss &gt; 0:\n        loss = p.loss(X_ls, y_ls).item()\n        loss_vec.append(loss)\n        \n        # Store this update\n        updates.append({\n            'old_w': old_w.clone(),\n            'new_w': p.w.clone(),\n            'point_idx': i,\n            'loss': loss\n        })\n\n# If we have more updates than plots, select a representative subset\nif len(updates) &gt; max_plots - 1:  # Save one spot for final result\n    # Choose evenly spaced updates, always including the first few and last one\n    indices = np.linspace(0, len(updates)-1, max_plots-1, dtype=int)\n    selected_updates = [updates[i] for i in indices]\nelse:\n    selected_updates = updates\n\n# Plot the selected updates\nfor idx, update in enumerate(selected_updates):\n    if idx &lt; max_plots:\n        ax = axarr.ravel()[idx]\n        plot_perceptron_data(X_ls, y_ls, ax)\n        draw_line(update['old_w'], x_min=-1, x_max=2, ax=ax, color=\"black\", linestyle=\"dashed\")\n        draw_line(update['new_w'], x_min=-1, x_max=2, ax=ax, color=\"black\")\n        i = update['point_idx']\n        ax.scatter(X_ls[i,0], X_ls[i,1], color=\"black\", facecolors=\"none\", \n                  edgecolors=\"black\", marker=markers[marker_map[y_ls[i].item()]])\n        ax.set_title(f\"loss = {update['loss']:.3f}\")\n        ax.set(xlim=(-1, 2), ylim=(-1, 2))\n\n# Always show the final result in the last subplot\nif current_ax &lt; max_plots and len(updates) &gt; 0:\n    ax = axarr.ravel()[min(len(selected_updates), max_plots-1)]\n    plot_perceptron_data(X_ls, y_ls, ax)\n    draw_line(p.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\n    ax.set_title(f\"Final: loss = {loss:.3f}\")\n    ax.set(xlim=(-1, 2), ylim=(-1, 2))\n\nplt.tight_layout()\nprint(f\"Total updates needed: {len(updates)}\")\nprint(f\"Final loss: {loss:.6f}\")\n\n\nTotal updates needed: 23\nFinal loss: 0.000000\n\n\n\n\n\n\n\n\n\nThese figures show how the decision boundary evolves each time the model encounters a misclassified point. With each update, the boundary shifts to correct the current error, sometimes overcorrecting slightly. This creates a zigzag pattern of adjustments as the model alternates between fixing errors on opposite sides of the boundary, gradually converging to a position that correctly classifies all points.\n\n\nPart B.2: Seeing how the Model does on Non-Linearly Separable Data\nThe perceptron algorithm will not converge to a decision boundary on non-linearly separable data. To show this, let’s first create a dataset that cannot be perfectly separated by a linear boundary. To do this, we’ll use the same method as before but simply increase the noise level. As you’ll see, it’s clearly imposisble to separate the data with a single line.\n\n\nCode\ntorch.manual_seed(42)\n\n# create non-linearly separable data\nX_nls, y_nls = perceptron_data(n_points = 50, noise = 0.8)\n\n# plot non-linearly separable data\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\nax.set_title(\"Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\n\nNow that we have our non-linearly separable data, we can run the perceptron algorithm on it and see what happens. Since it will not converge, we can termiante the loop after a set number of iterations. We’ll do this for 1000 iterations.\n\n\nCode\ntorch.manual_seed(42)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\nloss_vec = []\n\niter = 0\n\nwhile loss &gt; 0: \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_nls, y_nls) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_nls, y_nls)\n\n    # update iter\n    iter += 1\n\n    # maxiter condition\n    if iter &gt;= 1000:\n        break\n\n\nWe can create the same plot as before to visualize the loss changes:\n\n\nCode\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\")\n\n\nText(0.5, 1.0, 'Loss Function Throughout Model Training')\n\n\n\n\n\n\n\n\n\nLooking at the figure above, it’s clear that my model fails to converge. The perceptron algorithm’s sensitivity to individual data points causes the loss to fluctuate dramatically throughout the process, swinging between values below of about 0.1 and over 0.7.\nFor the linearly separable case, we previously examined each update to the weight vector. However, due to the sheer number of updates in this scenario, inspecting every change would be impractical. Instead, I present the decision boundary from the model’s final iteration:\n\n\nCode\n# Plot final decision boundary\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(p.w, x_min = -1.5, x_max = 2.5, ax = ax, color = \"black\")\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\n\nThis decision boundary doesn’t look terrible. A majority of the blue points are above, while most of the orange points are below. However, there is definitely some misclassification, highlting perceptron’s weekness on non-linearly separable data.\n\n\nPart B.3: On 5-Dimensions\nTo show that the perceptron algorithm works in any number of dimensions, I will generate a random dataset in 5 dimensions and run the perceptron algorithm on it.\n\n\nCode\ntorch.manual_seed(1234)\n\n# create data\nX_5d, y_5d = perceptron_data(n_points = 50, noise = 0.3, p_dims = 5)\n\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\nloss_vec = []\n\niter = 0\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    loss = p.loss(X_5d, y_5d) \n    loss_vec.append(loss)\n    \n    # perform a perceptron update using the random data point\n    opt.step(X_5d, y_5d)\n\n    iter += 1\n    if iter &gt;= 1000:\n        break\n\n\nAnd let’s take a look at the loss function:\n\n\nCode\n# plot the changes in loss \nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"Loss\")\nplt.title(\"Loss Function Throughout Model Training\")\n\n\nText(0.5, 1.0, 'Loss Function Throughout Model Training')\n\n\n\n\n\n\n\n\n\nIt looks that we terminate (achieve a loss of 0) meaning this 5d data is linearly separable. This is pretty cool that we can converge even in 5 dimensions."
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html#part-c-minibatch-perceptron",
    "href": "posts/perceptron-blog/perceptron-blog.html#part-c-minibatch-perceptron",
    "title": "Perceptron",
    "section": "Part C: Minibatch Perceptron",
    "text": "Part C: Minibatch Perceptron\nThe minibatch perceptron is a variation of the perceptron algorithm that processes k observations per iteration instead of just one. I will implement this algorithm and run several experiments to explore its capabilities and limitations.\n\nPart C.1: Experiment 1 (k=1)\nFirst, I’ll run the minibatch perceptron with \\(k=1\\) and see how it performs.\n\n\nCode\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\n\n\n\nCode\ntorch.manual_seed(1234)\n\nX, y = perceptron_data()\n\np = MinibatchPerceptron()\nopt = MinibatchPerceptronOptimizer(p)\nloss = 1.0\n\nk = 1\n\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    # random submatrix of feature matrix passed to PerceptronOptimizer.step()\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n\n\n\nCode\n# plot the loss function\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\", s = 3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nThis looks like the minibatch perceptron is performing the similarly to the standard perceptron. We see a decreasing loss function, and the model converges to a loss of 0.\n\n\nPart C.2: Experiment 1 (k=10)\nLet’s see how the minibatch perceptron performs with \\(k=10\\).\n\n\nCode\ntorch.manual_seed(1234)\n\nX, y = perceptron_data()\n\np = MinibatchPerceptron()\nopt = MinibatchPerceptronOptimizer(p)\nloss = 1.0\n\n# set k = 10\nk = 10\n\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    ix = torch.randperm(X.size(0))[:k] # random submatrix of feature matrix\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n\n\n\nCode\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\", s = 3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nThis looks similar to the previous experiment. The loss function is decreasing, and the model converges to a loss of 0.\n\n\nPart C.3: Experiment 1 (k=n)\n\n\nCode\ntorch.manual_seed(1234)\n\nX, y = X_nls, y_nls\n\np = MinibatchPerceptron()\nopt = MinibatchPerceptronOptimizer(p)\nloss = 1.0\n\nk = len(X)\n\nloss_vec = []\n\nn = X.size()[0]\n\nfor i in range (0,1000):\n    # random submatrix of feature matrix passed to PerceptronOptimizer.step()\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n\n\n\nCode\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\", s = 3)\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\n\nWhen \\(k=n\\) with data that is not linearly separable, the loss converges to about 0.25. And the resulting decision boundary:\n\n\nCode\nfig, ax = plt.subplots(figsize = (5, 5))\nplot_perceptron_data(X_nls, y_nls, ax)\ndraw_line(p.w, x_min = -1.5, x_max = 2.5, ax = ax, color = \"black\")\nax.set_title(\"Decision Boundary on Non-Linearly Separable Data\");\n\n\n\n\n\n\n\n\n\nThis line looks pretty good. The decision boundary is able to separate the data, but not perfectly."
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html#part-d-runtime-analysis",
    "href": "posts/perceptron-blog/perceptron-blog.html#part-d-runtime-analysis",
    "title": "Perceptron",
    "section": "Part D: Runtime Analysis",
    "text": "Part D: Runtime Analysis\nThe update in each iteration of the perceptron algorithm is computed as \\(X \\times w\\), representing the product between a single row of the matrix \\(X\\) and the weight vector \\(w\\). Each row of \\(X\\) contains \\(p\\) entries corresponding to the \\(p\\) features, resulting in a time complexity of \\(O(p)\\).\nIn my mini-batch perceptron algorithm, the dot product is computed for each data point in the batch of size \\(k\\), leading to a runtime of \\(O(kp)\\) per iteration."
  },
  {
    "objectID": "posts/perceptron-blog/perceptron-blog.html#conclusion",
    "href": "posts/perceptron-blog/perceptron-blog.html#conclusion",
    "title": "Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this assignment, I explored the perceptron algorithm by implementing it from scratch. I demonstrated that the algorithm successfully converges on linearly separable data in any finite number of dimensions but fails to converge on non-linearly separable data. To address this limitation, I implemented the minibatch perceptron algorithm. I observed that when \\(k = 1\\), the minibatch perceptron behaves similarly to the standard perceptron. As \\(k\\) increases, the algorithm continues to find decision boundaries for linearly separable data. Notably, when \\(k = n\\) (where \\(n\\) is the total number of data points), the algorithm can converge to an imperfect solution even on non-linearly separable data. This assignment gave me valuable experience in implementing my first machine learning model from scratch and analyzing its performance and limitations."
  },
  {
    "objectID": "posts/replication/replication.html",
    "href": "posts/replication/replication.html",
    "title": "Replication Study",
    "section": "",
    "text": "In this replication study, I examine the findings of Obermeyer et al. (2019) in their paper “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Their research investigates potential racial bias in healthcare algorithms that identify high-risk patients for additional support services. My analysis reproduces key figures from their work that demonstrate disparities between Black and White patients in risk score assignment and program eligibility. Specifically, I recreate visualizations showing the relationship between risk scores, chronic condition burden, and healthcare costs across racial groups. Additionally, I conduct a linear regression analysis to quantify the healthcare cost differential between Black and White patients with comparable health status. Finally, I discuss the implications of these disparities through the lens of algorithmic fairness principles, identifying which fairness criteria are violated by the observed patterns.\n\n\nCode\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\nCode\ndf_copy = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_copy.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1"
  },
  {
    "objectID": "posts/replication/replication.html#abstract",
    "href": "posts/replication/replication.html#abstract",
    "title": "Replication Study",
    "section": "",
    "text": "In this replication study, I examine the findings of Obermeyer et al. (2019) in their paper “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Their research investigates potential racial bias in healthcare algorithms that identify high-risk patients for additional support services. My analysis reproduces key figures from their work that demonstrate disparities between Black and White patients in risk score assignment and program eligibility. Specifically, I recreate visualizations showing the relationship between risk scores, chronic condition burden, and healthcare costs across racial groups. Additionally, I conduct a linear regression analysis to quantify the healthcare cost differential between Black and White patients with comparable health status. Finally, I discuss the implications of these disparities through the lens of algorithmic fairness principles, identifying which fairness criteria are violated by the observed patterns.\n\n\nCode\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\nCode\ndf_copy = df[[\"risk_score_t\", \"cost_t\", \"race\", \"gagne_sum_t\"]].copy()\ndf_copy.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\nrace\ngagne_sum_t\n\n\n\n\n0\n1.987430\n1200.0\nwhite\n0\n\n\n1\n7.677934\n2600.0\nwhite\n3\n\n\n2\n0.407678\n500.0\nwhite\n0\n\n\n3\n0.798369\n1300.0\nwhite\n0\n\n\n4\n17.513165\n1100.0\nwhite\n1"
  },
  {
    "objectID": "posts/replication/replication.html#reproduction-of-figure-1-percentile-risk-score-vs.-mean-number-of-chronic-illnesses",
    "href": "posts/replication/replication.html#reproduction-of-figure-1-percentile-risk-score-vs.-mean-number-of-chronic-illnesses",
    "title": "Replication Study",
    "section": "Reproduction of Figure 1: Percentile Risk Score vs. Mean Number of Chronic Illnesses",
    "text": "Reproduction of Figure 1: Percentile Risk Score vs. Mean Number of Chronic Illnesses\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_copy['risk_percentile'] = round(df_copy['risk_score_t'].rank(pct = True), 2) * 100\n\ngrouped_data = df_copy.groupby(['risk_percentile', 'race'])['gagne_sum_t'].mean()\n\ngrouped_data = pd.DataFrame(grouped_data)\n\n# Use both hue and style parameters to control color and marker shape\nsns.scatterplot(data=grouped_data, x='gagne_sum_t', y='risk_percentile', \n                hue='race', style='race',\n                markers={'white': 'X', 'black': 'o'},\n                palette={'white': 'purple', 'black': 'teal'},\n                alpha=0.5)\n\nplt.xlabel('Mean Number of Active Chronic Conditions')\nplt.ylabel('Percentile of Algorithm Risk Score')\nplt.title('Mean number of chronic conditions by race, plotted against algorithm risk score')\n\nplt.legend(title='Race')\n\n\n\n\n\n\n\n\n\nThis plot begins to show some underlying racial bias that could be present within the algorithm. The white patients’ curve of percentile risk score is higher than the black patients’ curve. This suggests that less sick white patients are more likely to be referred to a high-risk program than sicker black patients.\n\nReproduction of Figure 3: Costs versus algorithm-predicted risk, and costs versus health, by race.\n\n\nCode\nfig, axes = plt.subplots(1, 2, sharey = True, figsize = (12, 5))\n\ncost_data = pd.DataFrame(df_copy.groupby(['risk_percentile', 'race'])['cost_t'].mean())\nillness_data = pd.DataFrame(df_copy.groupby(['gagne_sum_t', 'race'])['cost_t'].mean())\n\nsns.scatterplot(ax = axes[0], data=cost_data, x='risk_percentile', y='cost_t', hue='race', style='race', markers={'white': 'X', 'black': 'o'}, palette={'white': 'purple', 'black': 'teal'}, alpha=0.5)\nsns.scatterplot(ax = axes[1], data=illness_data, x='gagne_sum_t', y='cost_t', hue='race', style='race', markers={'white': 'X', 'black': 'o'}, palette={'white': 'purple', 'black': 'teal'}, alpha=0.5)\n\naxes[0].set_ylabel('Total Medical Expenditure')\naxes[0].set_xlabel('Percentile Risk Score')\naxes[1].set_xlabel('Number of Chronic Conditions')\n\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\nThis visualization reveals a clear racial disparity in healthcare costs. At equivalent levels of either risk score or chronic illness burden, Black patients consistently incur lower medical expenditures than White patients. This cost differential persists across most of the distribution, though the relationship becomes more volatile at the extreme right of both graphs where sample sizes diminish.\n\n\nModeling Cost Disparity\nThere isn’t much data about patients with 5 or less chronic illnesses. Thus, let’s limit the dataset to only patients with 5 or less chronic illnesses to prevent outliers.\n\n\nCode\nmore_than_5 = round((df_copy['gagne_sum_t'] &gt;= 5).mean() * 100, 2)\nmore_than_5\n\n\n7.0\n\n\nSince 93% of patients in this dataset have 5 or fewer chronic conditions, focusing on this majority group provides a reasonable choice for analysis. While examining trends for patients with higher chronic illness burdens remains important, limiting our scope to this predominant group helps avoid potential misinterpretations from sparse data. This approach ensures our analysis captures the most representative patterns while acknowledging the limitations of drawing conclusions about smaller subgroups with more complex conditions.\n\n\nData Prep\nBelow I calculate the log cost. This is important because our target variable varies widely across several orders of magnitude.\n\n\nCode\nimport numpy as np\n\ndf_not_0 = df_copy.drop(df_copy[df_copy['cost_t'] == 0].index)\ndf_not_0['log_cost'] = np.log(df_not_0['cost_t'])\n\n\nNext, I create dummy variables for the race column.\n\n\nCode\ndf_not_0 = pd.get_dummies(df_not_0)\ndf_not_0.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\ncost_t\ngagne_sum_t\nrisk_percentile\nlog_cost\nrace_black\nrace_white\n\n\n\n\n0\n1.987430\n1200.0\n0\n35.0\n7.090077\nFalse\nTrue\n\n\n1\n7.677934\n2600.0\n3\n86.0\n7.863267\nFalse\nTrue\n\n\n2\n0.407678\n500.0\n0\n4.0\n6.214608\nFalse\nTrue\n\n\n3\n0.798369\n1300.0\n0\n11.0\n7.170120\nFalse\nTrue\n\n\n4\n17.513165\n1100.0\n1\n98.0\n7.003065\nFalse\nTrue\n\n\n\n\n\n\n\n\n\nCode\nX_train = df_not_0[['gagne_sum_t', 'race_black']].rename(columns={'race_black' : 'race'})\ny_train = df_not_0[['log_cost']]"
  },
  {
    "objectID": "posts/replication/replication.html#model-logistic-regression",
    "href": "posts/replication/replication.html#model-logistic-regression",
    "title": "Replication Study",
    "section": "Model: Logistic Regression",
    "text": "Model: Logistic Regression\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\ndef add_polynomial_features(X, degree):\n  X_ = X.copy()\n  for j in range(1, degree):\n    X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"]**j\n  return X_\n\n\nLet’s now use cross validation to see how many polynomial features are best.\n\n\nCode\nfrom sklearn.linear_model import Ridge\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nimport warnings\n\npoly_degrees = range(1, 12)  # From 1 to 11\nalphas = [10**k for k in range(-4, 5)]  # 10^-4 to 10^4\n\nresults_list = []\n\n# Loop through polynomial degrees and regularization strengths\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    for degree in poly_degrees:\n        for alpha in alphas:\n            # Add polynomial features\n            X_deg = add_polynomial_features(X_train, degree)\n            \n            # Create and fit Ridge regression model\n            ridge = Ridge(alpha=alpha)\n            \n            # Calculate cross-validation score\n            cv_score = cross_val_score(ridge, X_deg, y_train, cv=5, \n                                      scoring='neg_mean_squared_error').mean()\n            \n            results_list.append({\n                'poly_degree': degree,\n                'alpha': alpha,\n                'cv_score': cv_score\n            })\n\nresults = pd.DataFrame(results_list)\n\n# Find the best combination (highest score = lowest MSE since scores are negative)\nbest_result = results.loc[results['cv_score'].idxmax()]\nprint(f\"Best polynomial degree: {best_result['poly_degree']}\")\nprint(f\"Best alpha: {best_result['alpha']}\")\nprint(f\"Best CV score (negative MSE): {best_result['cv_score']}\")\n\n# Find the best model\nbest_X = add_polynomial_features(X_train, int(best_result['poly_degree']))\nbest_model = Ridge(alpha=best_result['alpha'])\nbest_model.fit(best_X, y_train)\n\nw_ridge = best_model.coef_\nprint(\"Model coefficients:\")\nprint(w_ridge)\n\n# Extract the coefficient for race\ncoef_b = w_ridge[1]\nprint(f\"Coefficient for race: {coef_b}\")\n\n# Calculate the cost incurred by Black patients as a percentage of White patients\ncost_incurred_b = np.exp(coef_b)\nprint(f\"Cost incurred by Black patients as percentage of White patients: {cost_incurred_b:.2%}\")\n\n\nBest polynomial degree: 10.0\nBest alpha: 1.0\nBest CV score (negative MSE): -1.5084341103486478\nModel coefficients:\n[ 4.96332019e-01 -2.67045509e-01  4.96332015e-01 -9.84047253e-01\n  5.56357271e-01 -1.66928757e-01  2.91396071e-02 -3.04865853e-03\n  1.87779882e-04 -6.26253079e-06  8.70642175e-08]\nCoefficient for race: -0.2670455094431721\nCost incurred by Black patients as percentage of White patients: 76.56%\n\n\nThe code above performs a comprehensive search for the best model. I test different polynomial degrees (1-11) and regularization strengths (10^-4 to 10^4) using cross-validation to find the optimal combination that minimizes prediction error. After identifying the best model parameters, I fit the final model and extract the coefficient for race, which reveals how much less Black patients cost compared to White patients with equivalent health status.\nThe coefficients correspond to the order of features in our input data. The second coefficient represents the effect of race (race_black). This value can be interpreted as the estimated percentage difference in costs between Black patients and White patients, controlling for health status. This value can be interpreted as the estimated percentage difference in costs between Black patients and White patients, controlling for health status\nThe coefficient indicates that Black patients incur approximately 76.56% of the healthcare costs of White patients with equivalent health status. This finding aligns with the core argument presented by Obermeyer et al. (2019), though my analysis reveals a somewhat larger racial disparity in healthcare expenditures than documented in the original paper. Nevertheless, both analyses consistently demonstrate that Black patients generate lower healthcare costs than White patients with comparable health conditions."
  },
  {
    "objectID": "posts/replication/replication.html#discussion",
    "href": "posts/replication/replication.html#discussion",
    "title": "Replication Study",
    "section": "Discussion",
    "text": "Discussion\nThis replication study highlights the substantial effort required in data preparation and cleaning to achieve meaningful analytical results. The recreated Figure 1 demonstrates that white patients consistently receive higher algorithm risk scores than black patients with equivalent chronic illness burdens. This disparity creates a concerning situation where black patients are less frequently referred to beneficial high-risk care management programs despite similar health needs.\nFigure 3 clearly illustrates that across most of the distribution—particularly at lower chronic illness counts where data is most abundant—white patients generate higher healthcare expenditures than black patients with comparable health conditions. The linear regression analysis quantifies this disparity, confirming the systematic cost differential between racial groups.\nOf the formal statistical discrimination criteria discussed in Chapter 3 of Barocas, Hardt, and Narayanan (2023), which best describe the purported bias of the algorithm studied by Obermeyer et al. (2019)? What aspects of the study support your answer?\nOf the formal statistical discrimination criteria discussed in Chapter 3 of Barocas, Hardt, and Narayanan (2023), the separation criterion (error rate balance) most clearly describes the algorithmic bias identified by Obermeyer et al. (2019). While the algorithm demonstrates similar relationships between risk scores and costs across racial groups, it systematically assigns higher risk percentile scores to white patients compared to black patients with equivalent health conditions. This creates an imbalance in error rates, as Obermeyer et al. explicitly note that the algorithm incorrectly concludes that “black patients are healthier than equally sick white patients.” This disparity in risk assessment directly impacts healthcare access, with the algorithm effectively imposing a higher threshold for black patients to qualify for additional care programs despite similar underlying health needs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my Machine Learning blog for Spring 2025!"
  }
]