[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my Machine Learning blog for Spring 2025!"
  },
  {
    "objectID": "posts/blog-1/blog1.html",
    "href": "posts/blog-1/blog1.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this analysis, I explore the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their physical characteristics. Using a decision tree classifier with carefully selected features including culmen length, flipper length, and island location, I achieved high accuracy in distinguishing between Adelie, Chinstrap, and Gentoo penguins. Through exploratory data analysis and feature selection, I identified the most important predictors and optimized the model’s depth parameter. The final model achieved excellent performance on the test set, with only one misclassification out of all test cases - a Gentoo penguin incorrectly identified as an Adelie. This demonstrates that penguin species can be reliably classified using a small set of physical measurements and location data.\n\n\nWe start by importing the Palmer Penguins dataset. Take a look at the first few rows to begin to understand the data.\n\n\nCode\n# Importing the Palmer Penguins dataset\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nData preparation is essential. Some steps that we took here are: 1. Dropping the columns that are not relevant to our analysis, and then drop any rows that contain missing values. 2. Converting the species labels into a numerical format that can be used by the model. 3. Converting the categorical features into dummy variables. Now take a look at the first few rows of the transformed data.\n\n\nCode\n# Data Prep\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNow that we have the data in a usable format, I started to explore the various features and their relationships. The goal of this is to guage which features might be best to use for our model.\n\n\nCode\n## Explore\n# 2 interesting visualizations\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# 1. Distribution of Culmen Length by Species\np1 = sns.scatterplot(x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", data = train, ax = axes[0])\n\n# 2. Spread of Flipper Length by Sex\np2 = sns.violinplot(x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Sex_FEMALE\", data = X_train, ax = axes[1])\np2.set_xlabel(\"Classified as Female\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove I’ve created two visualizations to explore the data. The first shows the relationship between flipper length and culmen length for each species. The second shows the distribution of flipper length for each sex. I created the first plot because I was curious whether 1 physical characteristic might help predict the magnitude of another (ie, if a penguin has a larger flipper, does it tend to have a larger culmen?). Second, I wanted to look at the distrubutions of a physical characteristic by sex. Based on the violin plot, there is lots of overlap in flipper length between males and females, which helps us rule out sex as potential leading predictor of flipper length. And, further, this fact can likely be spread to other physical characteristics, meaning sex might not help us predict physical characteristics (and thus species) at all.\n\n\nCode\n# Summary Table: compute the average or median value of some features, by group\n\ntrain.groupby(\"Species\").aggregate({\"Body Mass (g)\": [\"mean\", \"std\"], \"Culmen Length (mm)\": [\"mean\", \"std\"], \"Culmen Depth (mm)\": [\"mean\", \"std\"], \"Flipper Length (mm)\": [\"mean\", \"std\"]}).round(2)\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n3718.49\n462.66\n38.97\n2.64\n18.41\n1.22\n190.08\n6.69\n\n\nChinstrap penguin (Pygoscelis antarctica)\n3743.42\n407.42\n48.83\n3.45\n18.37\n1.14\n196.00\n7.42\n\n\nGentoo penguin (Pygoscelis papua)\n5039.95\n498.86\n47.07\n2.74\n14.91\n1.00\n216.75\n5.93\n\n\n\n\n\n\n\n\n\n\n\nHere I’ve created a summary table that shows the average and standard deviation of body mass, culmen length, culmen depth, and flipper length for each species. This helps us become aquainted with the magnitude of our data and its distribution. Looking at the standard deviations, we can see that the relative spread of each respective characteristic is similar across species. And, right off the bat, there is not a clear unique physical characteristic that can be used to identify each species. The Gentoo penguin does quite largely have the largest body mass, but then has a similar culmen length to the Chinstrap penguin. This is important to note because it suggests that we will need to utilize multiple physical characteristics to accurately predict species.\n\n\n\nFeature selection is likely the most imporant step in this analysis, as the entirety of our model’s performance will depend on the predictive power of our features. Below I use the SelectKBest algorithm to select the top 2 quantitative features and the top qualitative feature.\n\n\nCode\n## Features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Separate qualitataive and quantitative features\nqualitative_features = [\"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\nquantitative_features = [col for col in X_train.columns if col not in qualitative_features]\n\n# Select 2 quantitative features\nquant_selector = SelectKBest(f_classif, k = 2)\nX_train_quant = X_train[quantitative_features]\nquant_selector.fit(X_train_quant, y_train)\nX_new_train = quant_selector.transform(X_train_quant)\nselected_quant_features = X_train_quant.columns[quant_selector.get_support()]\n\n\n# Select 1 qualitataive features\nqual_selector = SelectKBest(f_classif, k = 3)\nX_train_qual = X_train[qualitative_features]\nqual_selector.fit(X_train_qual, y_train)\nX_new_train = qual_selector.transform(X_train_qual)\nselected_qual_feature = X_train_qual.columns[qual_selector.get_support()]\n\n# Combine selected quantitative and qualitative features\nfinal_features = list(selected_quant_features) + list(selected_qual_feature)\nX_new_train = X_train[final_features]\nprint(X_new_train.head(3))\n\n# Top 3 features are Culmen Length (mm)  Flipper Length (mm)  and Island.\n\n\n   Culmen Length (mm)  Flipper Length (mm)  Island_Biscoe  Island_Dream  \\\n0                40.9                187.0          False          True   \n1                49.0                210.0          False          True   \n2                50.0                218.0           True         False   \n\n   Island_Torgersen  \n0             False  \n1             False  \n2             False  \n\n\n\n\n\nI use DecisionTreeClassifier to create a baseline model. I utilize the top 3 features that we selected above to train the model and then evaluate its performance on the training set. We achieve 100% accuracy.\n\n\nCode\n# Initial model with top 3 features. Reached 100% accuracy on training set.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_new_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_new_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\n\n\nTraining Accuracy: 1.000\n\n\n\n\n\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 7, and this optimized model also achieves 100% accuracy on the training set.\n\n\nCode\n# Now to utilize the max_depth parameter to see if we can improve by finding the optimal depth\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 31)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_new_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_new_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_new_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\n\n\n\nOptimal Depth: 7\nTraining Accuracy with Optimal Depth: 1.000\n\n\n\n\n\nNow we evaluate the performance of our model on the test set. We achieve 98.5% accuracy, missing only 1 penguin.\n\n\nCode\n# Test model\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test_new = X_test[final_features]  # Use the same features selected for training\ntest_accuracy = dt_classifier_optimal.score(X_test_new, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\n\n\nTest Accuracy: 0.985\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(dt_classifier_optimal, X_new_train, y_train)\nplot_regions(dt_classifier_optimal, X_test_new, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two 3 ply plots show the predictions of our model on the training and test sets. The model is able to predict the species of the penguin with high accuracy on both sets by using flipper length, culmen length, and the island location. We can see the 1 missclassified penguin in Figure 5.1, shown by the red dot in the blue region.\n\n\n\nA confusion matrix is a great way to illustrate performance, showing the number of correct and incorrect predictions for each species. I created a plot to simply show this, where the shade of the cell represents the number of penguins who were classified as that species.\n\n\nCode\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = dt_classifier_optimal.predict(X_test_new)\nC = confusion_matrix(y_test, y_test_pred)\nC\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n# Enjoy a nice visualization in the form of a heatmap\nplt.figure(figsize=(10, 8))\n\n# heatmap\nsns.heatmap(C, \n            annot=True,  # show numbers in cells\n            fmt='d',     # use integer format\n            cmap='Purples',\n            xticklabels=le.classes_,  # use species names\n            yticklabels=le.classes_)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Species')\nplt.ylabel('True Species')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/blog-1/blog1.html#abstract",
    "href": "posts/blog-1/blog1.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "In this analysis, I explore the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their physical characteristics. Using a decision tree classifier with carefully selected features including culmen length, flipper length, and island location, I achieved high accuracy in distinguishing between Adelie, Chinstrap, and Gentoo penguins. Through exploratory data analysis and feature selection, I identified the most important predictors and optimized the model’s depth parameter. The final model achieved excellent performance on the test set, with only one misclassification out of all test cases - a Gentoo penguin incorrectly identified as an Adelie. This demonstrates that penguin species can be reliably classified using a small set of physical measurements and location data.\n\n\nWe start by importing the Palmer Penguins dataset. Take a look at the first few rows to begin to understand the data.\n\n\nCode\n# Importing the Palmer Penguins dataset\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nData preparation is essential. Some steps that we took here are: 1. Dropping the columns that are not relevant to our analysis, and then drop any rows that contain missing values. 2. Converting the species labels into a numerical format that can be used by the model. 3. Converting the categorical features into dummy variables. Now take a look at the first few rows of the transformed data.\n\n\nCode\n# Data Prep\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n\nNow that we have the data in a usable format, I started to explore the various features and their relationships. The goal of this is to guage which features might be best to use for our model.\n\n\nCode\n## Explore\n# 2 interesting visualizations\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 8))\n\n# 1. Distribution of Culmen Length by Species\np1 = sns.scatterplot(x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", data = train, ax = axes[0])\n\n# 2. Spread of Flipper Length by Sex\np2 = sns.violinplot(x = \"Sex_FEMALE\", y = \"Flipper Length (mm)\", hue = \"Sex_FEMALE\", data = X_train, ax = axes[1])\np2.set_xlabel(\"Classified as Female\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAbove I’ve created two visualizations to explore the data. The first shows the relationship between flipper length and culmen length for each species. The second shows the distribution of flipper length for each sex. I created the first plot because I was curious whether 1 physical characteristic might help predict the magnitude of another (ie, if a penguin has a larger flipper, does it tend to have a larger culmen?). Second, I wanted to look at the distrubutions of a physical characteristic by sex. Based on the violin plot, there is lots of overlap in flipper length between males and females, which helps us rule out sex as potential leading predictor of flipper length. And, further, this fact can likely be spread to other physical characteristics, meaning sex might not help us predict physical characteristics (and thus species) at all.\n\n\nCode\n# Summary Table: compute the average or median value of some features, by group\n\ntrain.groupby(\"Species\").aggregate({\"Body Mass (g)\": [\"mean\", \"std\"], \"Culmen Length (mm)\": [\"mean\", \"std\"], \"Culmen Depth (mm)\": [\"mean\", \"std\"], \"Flipper Length (mm)\": [\"mean\", \"std\"]}).round(2)\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\n3718.49\n462.66\n38.97\n2.64\n18.41\n1.22\n190.08\n6.69\n\n\nChinstrap penguin (Pygoscelis antarctica)\n3743.42\n407.42\n48.83\n3.45\n18.37\n1.14\n196.00\n7.42\n\n\nGentoo penguin (Pygoscelis papua)\n5039.95\n498.86\n47.07\n2.74\n14.91\n1.00\n216.75\n5.93\n\n\n\n\n\n\n\n\n\n\n\nHere I’ve created a summary table that shows the average and standard deviation of body mass, culmen length, culmen depth, and flipper length for each species. This helps us become aquainted with the magnitude of our data and its distribution. Looking at the standard deviations, we can see that the relative spread of each respective characteristic is similar across species. And, right off the bat, there is not a clear unique physical characteristic that can be used to identify each species. The Gentoo penguin does quite largely have the largest body mass, but then has a similar culmen length to the Chinstrap penguin. This is important to note because it suggests that we will need to utilize multiple physical characteristics to accurately predict species.\n\n\n\nFeature selection is likely the most imporant step in this analysis, as the entirety of our model’s performance will depend on the predictive power of our features. Below I use the SelectKBest algorithm to select the top 2 quantitative features and the top qualitative feature.\n\n\nCode\n## Features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Separate qualitataive and quantitative features\nqualitative_features = [\"Sex_FEMALE\", \"Sex_MALE\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\", \"Stage_Adult, 1 Egg Stage\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\nquantitative_features = [col for col in X_train.columns if col not in qualitative_features]\n\n# Select 2 quantitative features\nquant_selector = SelectKBest(f_classif, k = 2)\nX_train_quant = X_train[quantitative_features]\nquant_selector.fit(X_train_quant, y_train)\nX_new_train = quant_selector.transform(X_train_quant)\nselected_quant_features = X_train_quant.columns[quant_selector.get_support()]\n\n\n# Select 1 qualitataive features\nqual_selector = SelectKBest(f_classif, k = 3)\nX_train_qual = X_train[qualitative_features]\nqual_selector.fit(X_train_qual, y_train)\nX_new_train = qual_selector.transform(X_train_qual)\nselected_qual_feature = X_train_qual.columns[qual_selector.get_support()]\n\n# Combine selected quantitative and qualitative features\nfinal_features = list(selected_quant_features) + list(selected_qual_feature)\nX_new_train = X_train[final_features]\nprint(X_new_train.head(3))\n\n# Top 3 features are Culmen Length (mm)  Flipper Length (mm)  and Island.\n\n\n   Culmen Length (mm)  Flipper Length (mm)  Island_Biscoe  Island_Dream  \\\n0                40.9                187.0          False          True   \n1                49.0                210.0          False          True   \n2                50.0                218.0           True         False   \n\n   Island_Torgersen  \n0             False  \n1             False  \n2             False  \n\n\n\n\n\nI use DecisionTreeClassifier to create a baseline model. I utilize the top 3 features that we selected above to train the model and then evaluate its performance on the training set. We achieve 100% accuracy.\n\n\nCode\n# Initial model with top 3 features. Reached 100% accuracy on training set.\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_classifier = DecisionTreeClassifier(random_state = 42)\ndt_classifier.fit(X_new_train, y_train)\n\ntrain_accuracy = dt_classifier.score(X_new_train, y_train)\nprint(f\"Training Accuracy: {train_accuracy:.3f}\")\n\n\nTraining Accuracy: 1.000\n\n\n\n\n\nNow I utilize the max_depth parameter to see if we can improve by finding the optimal depth. I use cross-validation to evaluate the performance of the model at each depth. We find that the optimal depth is 7, and this optimized model also achieves 100% accuracy on the training set.\n\n\nCode\n# Now to utilize the max_depth parameter to see if we can improve by finding the optimal depth\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\npotential_depths =  range(1, 31)\nmean_scores = []\n\nfor depth in potential_depths:\n    classifier_instance = DecisionTreeClassifier(max_depth = depth, random_state = 42)\n    scores = cross_val_score(classifier_instance, X_new_train, y_train, cv = 5)\n    mean_scores.append(scores.mean())\n\noptimal_depth = potential_depths[np.argmax(mean_scores)]\n\nprint(f\"Optimal Depth: {optimal_depth}\")\n\n# Train new model with optimal depth\ndt_classifier_optimal = DecisionTreeClassifier(max_depth = optimal_depth, random_state = 42)\ndt_classifier_optimal.fit(X_new_train, y_train)\n\ntrain_accuracy_optimal = dt_classifier_optimal.score(X_new_train, y_train)\nprint(f\"Training Accuracy with Optimal Depth: {train_accuracy_optimal:.3f}\")\n\n\n\n\n\nOptimal Depth: 7\nTraining Accuracy with Optimal Depth: 1.000\n\n\n\n\n\nNow we evaluate the performance of our model on the test set. We achieve 98.5% accuracy, missing only 1 penguin.\n\n\nCode\n# Test model\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nX_test_new = X_test[final_features]  # Use the same features selected for training\ntest_accuracy = dt_classifier_optimal.score(X_test_new, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.3f}\")\n\n\nTest Accuracy: 0.985\n\n\n\n\nCode\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (8, 4))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\ncols = [\"Culmen Length (mm)\", \"Flipper Length (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(dt_classifier_optimal, X_new_train, y_train)\nplot_regions(dt_classifier_optimal, X_test_new, y_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese two 3 ply plots show the predictions of our model on the training and test sets. The model is able to predict the species of the penguin with high accuracy on both sets by using flipper length, culmen length, and the island location. We can see the 1 missclassified penguin in Figure 5.1, shown by the red dot in the blue region.\n\n\n\nA confusion matrix is a great way to illustrate performance, showing the number of correct and incorrect predictions for each species. I created a plot to simply show this, where the shade of the cell represents the number of penguins who were classified as that species.\n\n\nCode\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = dt_classifier_optimal.predict(X_test_new)\nC = confusion_matrix(y_test, y_test_pred)\nC\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\n# Enjoy a nice visualization in the form of a heatmap\nplt.figure(figsize=(10, 8))\n\n# heatmap\nsns.heatmap(C, \n            annot=True,  # show numbers in cells\n            fmt='d',     # use integer format\n            cmap='Purples',\n            xticklabels=le.classes_,  # use species names\n            yticklabels=le.classes_)\n\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Species')\nplt.ylabel('True Species')\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 1 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 25 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua)."
  },
  {
    "objectID": "posts/blog-1/blog1.html#discussion",
    "href": "posts/blog-1/blog1.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThis analysis demonstrates the effectiveness of using decision trees for penguin species classification. Through careful feature selection, we identified that just three key features - culmen length, flipper length, and island location - were sufficient to achieve nearly perfect classification accuracy. In identifying the most useful features by using the SelectKBest algorithm (2 quantitative, 1 qualitative), we were able to maximize our model efficiency and reach a high performance.\nThe exploratory data analysis revealed interesting patterns in penguin morphology. The scatter plots showed clear clustering of species based on physical characteristics, though with some overlap, particularly between Adelie and Chinstrap penguins. The violin plots examining sex differences in flipper length suggested that sex was not a strong predictor of physical characteristics, which helped inform our feature selection process.\nOur final model achieved high accuracy, misclassifying only one Gentoo penguin as an Adelie in the test set. This single error occurred on Biscoe Island, suggesting that while location is helpful for classification, it should not be relied upon exclusively. The confusion matrix visualization clearly showed this isolated error while highlighting the model’s otherwise perfect performance.\nOne key aspect that I learned from this project was the importance sufficient explortion, both in feature selection and model parameter tuning. It would be easy to pick 3 features that may look promising after pre-analyis plotting, but following a precise process to truly discover which features are optimal is critical and allows us to not solely rely on the confidence of our models based on their ultimate accuracy, but also know that we chose the best possible measurements of prediction. This extends to finding the optimal depth to use in the DecisionTreeClassifier model. Discovering the best depth to use adds another layer of assurance when relying on it to make predictions.\nFuture work could explore whether this high accuracy holds for larger datasets or different penguin populations. Are these results externally valid? Or are the features we chose reliant on the speciifc type of penguin that we measured in this blog post?Additionally, investigating whether simpler models (like logistic regression) could achieve similar performance might be worthwhile, potentially offering more interpretable results while maintaining accuracy."
  },
  {
    "objectID": "posts/blog-2/blog-2.html",
    "href": "posts/blog-2/blog-2.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Introduction\nThis blog tackles a real-world problem: given information about an individual and their specific loan request, should a bank offer this person a loan? I conducted an exploration of the data to determine which features are the best predictors of loan status (whether someone will default or not). Using logistic regression, I determined that person_home_ownership and loan_percent_income were the most predictive features and created a weight vector for a linear score function.\nWith the goal of maximizing bank profit, I established an optimal threshold value: individuals with scores below this threshold are hypothetically offered loans, while those above are denied. However, as shown in my analysis, a model created solely to maximize profit can lead to disparate impacts across different groups, which will be discussed more at the conclusion of this post. The approval rates vary by age (with older applicants facing much lower approval rates), loan purpose (with medical loans having the lowest approval rates), and income level (with a clear advantage for higher-income applicants).\nWhile the model is effective at predicting defaults and maximizing profits, these findings raise important questions about fairness and access to credit that financial institutions should consider beyond pure profit maximization.\n\n\nCode\n# Import relevant packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\n\n\nGrab the Data\n\n\nCode\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train.dropna()\ndf_train\ndf_train = df_train[df_train['person_age'] &lt;= 100]\n\n\n\n\nExplore the Data\n\n\nCode\n# Quick summary table\nsummary_stats = df_train.groupby(['person_home_ownership']).agg({\n    'loan_amnt': ['mean', 'std'],\n    'person_income': ['mean', 'std'],\n    'person_age': ['mean', 'std'],\n    'loan_percent_income': ['mean', 'std'],\n    'loan_int_rate': ['mean', 'std']\n})\n\nsummary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\nsummary_stats = summary_stats.reset_index()\n\nsummary_stats = summary_stats.rename(columns={\n    'loan_amnt_mean': 'Loan Amount (Mean)',\n    'loan_amnt_std': 'Loan Amount (Std)',\n    'person_income_mean': 'Income (Mean)',\n    'person_income_std': 'Income (Std)',\n    'person_age_mean': 'Age (Mean)',\n    'person_age_std': 'Age (Std)',\n    'loan_percent_income_mean': 'Loan % Income (Mean)',\n    'loan_percent_income_std': 'Loan % Income (Std)',\n    'loan_int_rate_mean': 'Interest Rate (Mean)',\n    'loan_int_rate_std': 'Interest Rate (Std)'\n})\n\nfor col in summary_stats.columns:\n    if col != 'person_home_ownership':\n        summary_stats[col] = summary_stats[col].round(2)\nsummary_stats\n\n\n\n\n\n\n\n\n\nperson_home_ownership\nLoan Amount (Mean)\nLoan Amount (Std)\nIncome (Mean)\nIncome (Std)\nAge (Mean)\nAge (Std)\nLoan % Income (Mean)\nLoan % Income (Std)\nInterest Rate (Mean)\nInterest Rate (Std)\n\n\n\n\n0\n0\n11305.19\n6138.49\n82145.16\n92740.81\n27.22\n6.58\n0.19\n0.11\n12.06\n2.71\n\n\n1\n1\n8912.19\n5797.05\n55567.60\n39786.38\n27.52\n6.32\n0.18\n0.11\n11.45\n3.12\n\n\n2\n2\n10620.18\n6779.92\n81684.78\n84164.30\n28.01\n6.40\n0.15\n0.10\n10.53\n3.30\n\n\n3\n3\n9051.96\n6218.84\n58930.36\n53365.47\n27.67\n6.07\n0.18\n0.11\n10.95\n3.23\n\n\n\n\n\n\n\nTable 1. Displays summary statistics for some key variables grouped by home ownership status. Home ownership classification: 0 = Other, 1 = Rent, 2 = Mortgage, 3 = Own.\n\n\nCode\n# Visualization #1\nplt.figure(figsize=(12, 6))\n\nsns.violinplot(data=df_train, \n               x='loan_intent', \n               y='person_age',\n               hue='loan_intent',\n               palette='viridis',\n               inner='box') \n\nplt.title('Age Distribution by Loan Intent', pad=15, fontsize=14)\nplt.xlabel('Loan Purpose', fontsize=12)\nplt.ylabel('Age', fontsize=12)\n\nplt.xticks(rotation=30)\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 1. Shows how the distributions of loan intent vary by age. Medical and personal loans have the largest age spread and education seems to have the lowest median age.\n\n\nCode\n# Visualization #2\n# Create income categories for better visualization\ndf_train['income_category'] = pd.qcut(df_train['person_income'], \n                                    q=5, \n                                    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n\nplt.figure(figsize=(12, 6))\n\nsns.boxplot(data=df_train, \n           x='income_category', \n           y='loan_amnt',\n           hue='person_home_ownership',\n           palette='deep')\n\nplt.title('Loan Amounts by Income Level and Home Ownership', pad=15, fontsize=14)\nplt.xlabel('Income Category', fontsize=12)\nplt.ylabel('Loan Amount ($)', fontsize=12)\n\n# format y-axis to show thousands\nplt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.legend(title='Home Ownership', bbox_to_anchor=(1.05, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFigure 2. Shows loan amounts across different income levels and home ownership status, which can help us understand who gets access to larger loans. I created 5 income brackets (separated the income category into 5 categories) to help show which segments of perspective borrowers have access to larger loans. This gives us perspective on the magnitude of loans, giving us some insight into who might be more likely to default (for example, maybe the outliers in the “Very Low” bracket are likely to default, as they are quite largely above the median loan amount for the rest of their bracket).\n\n\nBuild a Model\n\n\nCode\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_train = df_train.drop(columns=['income_category'])\n\nlabel_encoder = LabelEncoder()\n\nqual_features = [\n    'loan_intent',\n    'cb_person_default_on_file'\n]\n\nfor label in qual_features:\n    df_train[label] = label_encoder.fit_transform(df_train[label])\n\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OTHER', 0)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('RENT', 1)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('MORTGAGE', 2)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OWN', 3)\n\ny_train = df_train[\"loan_status\"]\nX_train = df_train.drop(columns=['loan_status', 'loan_grade'])\nX_train\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\n1\n3.0\n1\n11750\n13.47\n0.12\n1\n6\n\n\n2\n22\n36996\n1\n5.0\n1\n10000\n7.51\n0.27\n0\n4\n\n\n3\n24\n26000\n1\n2.0\n3\n1325\n12.87\n0.05\n0\n4\n\n\n4\n29\n53004\n2\n2.0\n2\n15000\n9.63\n0.28\n0\n10\n\n\n6\n21\n21700\n1\n2.0\n2\n5500\n14.91\n0.25\n0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n2\n8.0\n1\n3000\n7.29\n0.02\n0\n17\n\n\n26060\n23\n48000\n1\n1.0\n5\n4325\n5.42\n0.09\n0\n4\n\n\n26061\n22\n60000\n1\n0.0\n3\n15000\n11.71\n0.25\n0\n4\n\n\n26062\n30\n144000\n2\n12.0\n4\n35000\n12.68\n0.24\n0\n8\n\n\n26063\n25\n60000\n1\n5.0\n1\n21450\n7.29\n0.36\n0\n4\n\n\n\n\n22907 rows × 10 columns\n\n\n\n\n\nFind best features\nBelow I work to find the best features for our logistic regression model. I experimented with different numbers of features, and found that 2 features yielded the best results. I fit the logistic regression model with a cross-validation of 5 folds for each combination of features.\n\n\nCode\n# Find best features\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\npd.set_option('max_colwidth', 10000)\nnum_features = 2\n\ncols = X_train.columns\n\nscore_df_columns = ['features', 'score', 'weights']\nscore_df = pd.DataFrame(columns = score_df_columns)\n\nfor features in combinations(cols, num_features):\n  features = list(features)\n  \n  LR = LogisticRegression(max_iter = 10000000000)\n  LR.fit(X_train[features], y_train)  \n  LRscore = cross_val_score(LR, X_train[features], y_train, cv = 5).mean()\n  score_df.loc[len(score_df.index)] = [features, LRscore, LR.coef_]  \n\nscore_df = score_df.sort_values(by='score', ascending=False).head(5)\nscore_df\n\n\n\n\n\n\n\n\n\nfeatures\nscore\nweights\n\n\n\n\n21\n[person_home_ownership, loan_percent_income]\n0.848736\n[[-1.009688163139443, 8.271969139654322]]\n\n\n36\n[loan_amnt, loan_percent_income]\n0.824071\n[[-6.038304758244034e-05, 10.192521499044894]]\n\n\n39\n[loan_int_rate, loan_percent_income]\n0.823373\n[[0.28965363840490665, 8.41798717832072]]\n\n\n14\n[person_income, loan_percent_income]\n0.822238\n[[-1.0897117986924622e-05, 7.210498494849672]]\n\n\n32\n[loan_intent, loan_percent_income]\n0.819269\n[[-0.11082528287162276, 8.256012358077584]]\n\n\n\n\n\n\n\nAs seen by the table above, the best two features are person_home_ownership and loan_percent_income. Now we can define our weights from these features.\n\n\nCode\n# create weight array\nw = score_df.iloc[0][\"weights\"][0]\nw\n\n\narray([-1.00968816,  8.27196914])\n\n\n\n\nFind a Threshold\n\n\nCode\n# Start by defining X_train with the features found above\nX_train = X_train[['person_home_ownership', 'loan_percent_income']]\n\nmodel = LogisticRegression(random_state=42)\nmodel.fit(X_train, y_train)\n\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(X_train, w)\n\n# Calculate profit for repaid loans \ndef profit_repaid(loan_amount, interest_rate):\n    return loan_amount * (1 + 0.25 * interest_rate)**10 - loan_amount\n\n# Calculate profit for defaulted loans\ndef profit_defaulted(loan_amount, interest_rate):\n    return loan_amount * (1 + 0.25 * interest_rate)**3 - 1.7 * loan_amount\n\n\nOur manipulated data:\n\n\nCode\nX_train.head()\n\n\n\n\n\n\n\n\n\nperson_home_ownership\nloan_percent_income\n\n\n\n\n1\n1\n0.12\n\n\n2\n1\n0.27\n\n\n3\n1\n0.05\n\n\n4\n2\n0.28\n\n\n6\n1\n0.25\n\n\n\n\n\n\n\n\n\nCode\n# Finding the optimal threshold\nthresholds = np.linspace(0, 1, 100)\nprofits_per_borrower = []\n\n# Get predicted probabilities from our model\n# y_prob_train has the predicted probability of default for each loan in training set\ny_prob_train = model.predict_proba(X_train)[:, 1] \n\nfor threshold in thresholds:\n    y_pred = (y_prob_train &gt;= threshold)\n\n    loan_amounts = df_train.loc[X_train.index, 'loan_amnt'] #get the loan amount for each loan in training set\n    interest_rates = df_train.loc[X_train.index, 'loan_int_rate'] / 100 #convert to decimal\n\n    # Calculate total profit (gains/losses)\n    # For predicted repaid loans (y_pred == 0): we use profits_repaid\n    # For predicted defaults (y_pred == 1): deny the loan, so profit is 0\n    profits_repaid = profit_repaid(loan_amounts, interest_rates)\n    profits_defaulted = profit_defaulted(loan_amounts, interest_rates)\n    \n    predicted_repay = (y_pred == 0)\n    \n    # only give loans to those predicted to repay\n    # for those predicted to default, we don't give loans, so profit is 0\n    total_profit = np.sum(profits_repaid * predicted_repay)\n    \n    # calculate what actually happens - some loans we approve will default (doesn't affect our decision making tho)\n    actual_defaults = (y_train == 1)\n    actual_repayments = (y_train == 0)\n    \n    # Calculate what we actually earn considering who actually defaults and who repays\n    # Among approved loans (predicted_repay):\n    # thosewho actually repay (actual_repayments): we get profits_repaid\n    # those who actually default (actual_defaults): we get profits_defaulted\n    actual_profit = np.sum(profits_repaid * (predicted_repay & actual_repayments)) + \\\n                    np.sum(profits_defaulted * (predicted_repay & actual_defaults))\n    \n    profit_per_borrower = actual_profit / len(X_train)\n    profits_per_borrower.append(profit_per_borrower)\n\n# get optimal threshold\noptimal_threshold = thresholds[np.argmax(profits_per_borrower)]\nmax_profit_per_borrower = np.max(profits_per_borrower)\n\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds, profits_per_borrower)\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', \n            label=f'Optimal threshold = {optimal_threshold:.3f}')\nplt.title('Profit per Borrower vs Classification Threshold')\nplt.xlabel('Threshold')\nplt.ylabel('Profit per Borrower ($)')\nplt.grid(False)\nplt.legend()\nplt.show()\n\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")\nprint(f\"Expected profit per borrower: ${max_profit_per_borrower:,.2f}\")\n\n# show confusion matrix at optimal threshold\ny_pred_optimal = (y_prob_train &gt;= optimal_threshold)\nprint(\"\\nConfusion Matrix at Optimal Threshold:\")\nprint(confusion_matrix(y_train, y_pred_optimal))\n\n# show approval rate\napproval_rate = np.mean(y_pred_optimal == 0) * 100\nprint(f\"\\nApproval rate: {approval_rate:.1f}%\")\n\n\n\n\n\n\n\n\n\n\nOptimal threshold: 0.535\nExpected profit per borrower: $1,450.63\n\nConfusion Matrix at Optimal Threshold:\n[[17863   118]\n [ 3253  1673]]\n\nApproval rate: 92.2%\n\n\nFigure 3. Shows the optimal threshold (profit maximizing) profit per borrower.\n\n\nEvaluate the Model from the Bank’s Perspective\n\n\nCode\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test =df_test.dropna()\n\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OTHER', 0)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('RENT', 1)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('MORTGAGE', 2)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OWN', 3)\n\nX_test = df_test[['person_home_ownership', 'loan_percent_income']]\n\n\n\n\nCode\n\ny_prob_test = model.predict_proba(X_test)[:, 1]\n\ny_pred_test = (y_prob_test &gt;= optimal_threshold)\n\nloan_amounts_test = df_test['loan_amnt']\ninterest_rates_test = df_test['loan_int_rate'] / 100\n\n# Calculate potential profits for test loans\nprofits_repaid_test = profit_repaid(loan_amounts_test, interest_rates_test)\nprofits_defaulted_test = profit_defaulted(loan_amounts_test, interest_rates_test)\n\n# Determine which loans we would approve\npredicted_repay_test = (y_pred_test == 0)\n\n# Calculate approval rate on test set\napproval_rate_test = np.mean(predicted_repay_test) * 100\nprint(f\"Test set approval rate: {approval_rate_test:.1f}%\")\n\n# If test set has actual outcomes (y_test), get performance\nif 'loan_status' in df_test.columns:\n    y_test = df_test['loan_status']\n    \n    # confusion matrix\n    print(\"\\nConfusion Matrix on Test Set:\")\n    print(confusion_matrix(y_test, y_pred_test))\n    \n    # calculate classification metrics\n    print(\"\\nClassification Report on Test Set:\")\n    print(classification_report(y_test, y_pred_test))\n    \n    # calculate actual profit on test set\n    actual_defaults_test = (y_test == 1)\n    actual_repayments_test = (y_test == 0)\n    \n    actual_profit_test = np.sum(profits_repaid_test * (predicted_repay_test & actual_repayments_test)) + \\\n                         np.sum(profits_defaulted_test * (predicted_repay_test & actual_defaults_test))\n    \n    profit_per_borrower_test = actual_profit_test / len(X_test)\n    print(f\"\\nExpected profit per borrower on test set: ${profit_per_borrower_test:,.2f}\")\nelse:\n    # If we don't have actual outcomes, just calculate expected profit based on model predictions\n    expected_profit = 0\n    for i, prob in enumerate(y_prob_test):\n        # Expected profit = (probability of repayment × profit if repaid) + \n        #                   (probability of default × profit if defaulted)\n        if predicted_repay_test[i]:  # If we approve this loan\n            expected_profit += (1-prob) * profits_repaid_test.iloc[i] + prob * profits_defaulted_test.iloc[i]\n    \n    expected_profit_per_borrower = expected_profit / len(X_test)\n    print(f\"\\nExpected profit per borrower on test set (based on model probabilities): ${expected_profit_per_borrower:,.2f}\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(y_prob_test, bins=50, alpha=0.7)\nplt.axvline(x=optimal_threshold, color='r', linestyle='--', \n            label=f'Optimal threshold = {optimal_threshold:.3f}')\nplt.title('Distrubution of the Default Probabilities in Test Set')\nplt.xlabel('Predicted Probability of Default')\nplt.ylabel('Number of Loans')\nplt.legend()\nplt.grid(False)\nplt.show()\n\n\nTest set approval rate: 91.6%\n\nConfusion Matrix on Test Set:\n[[4427   27]\n [ 824  453]]\n\nClassification Report on Test Set:\n              precision    recall  f1-score   support\n\n           0       0.84      0.99      0.91      4454\n           1       0.94      0.35      0.52      1277\n\n    accuracy                           0.85      5731\n   macro avg       0.89      0.67      0.71      5731\nweighted avg       0.87      0.85      0.82      5731\n\n\nExpected profit per borrower on test set: $1,394.87\n\n\n\n\n\n\n\n\n\nThe expected profit per borrower is $1,394.87. This is about $50 lower than the profit per borrower we predicted from the training set. The plot above shows the distribution of the predicted probabilities of defaulting in the test set and the number of loans given for each predicted probability. This is useful for understanding the ultimate duty of my model: as the probability of defaulting increases, we see the magnitude of loans given decreasing.\n\n\nEvaluate the Model From the Borrower’s Perspective\n\n\nIs it more difficult for people in certain age groups to access credit under your proposed system?\n\n\nCode\n# create age groups\nbins = [0, 30, 50, 70, 90, 150]\nlabels= [\"&lt;30\", \"30-50\", \"50-70\", \"70-90\", \"90+\"]\n\ndf_test['person_age_group'] = pd.cut(df_test[\"person_age\"], bins=bins, labels=labels, right=False)\n\ndf_test['expect_default'] = np.where(y_prob_test &gt;= optimal_threshold, 1, 0)\ndf_test['approved'] = 1 - df_test['expect_default']\n\napproval_by_age = df_test.groupby('person_age_group')['approved'].mean()\n\nprint(\"Approval rates by age group:\")\nprint(approval_by_age)\n\nplt.figure(figsize=(10, 6))\napproval_by_age.plot(kind='bar', color='skyblue')\nplt.title('Loan Approval Rates by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Approval Rate (%)')\nplt.legend()\nplt.grid(False)\nplt.tight_layout()\nplt.show()\n\n\nApproval rates by age group:\nperson_age_group\n&lt;30      0.911374\n30-50    0.930900\n50-70    0.888889\n70-90    0.500000\n90+           NaN\nName: approved, dtype: float64\n\n\n\n\n\n\n\n\n\nThis plot shows that certain age groups have different approval rates. We see that ages under 70 have relatively similar approval rates, but a sharp decline for the 70-90 age range and a 0 approval rate for those over 90. This is an interesting finding and showcases that older individuals have a much harder time accessing credit.\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.title(\"Distribution of Loans by Age\")\nsns.histplot(data = df_test, x = \"person_age\", hue = \"expect_default\", bins = 30).set(xlabel = \"age\", ylabel = \"count\")\n\n\n\n\n\n\n\n\n\nHere we see the actual count of the distribution of loans by age. This is essentially showing the same info as the plot above, but giving a number to the loan counts. We see that the distribution of loans follows the distribution of predicting a default, which intuitively makes sense. This shows that approving a loan is less dependent on age and more dependent on the probability of defaulting. But, since our algorthim depends on the data, age groups’ approval rates are biased by their age, not just their applicant status.\n\n\nIs it more difficult for people to get loans in order to pay for medical expenses? How does this compare with the actual rate of default in that group? What about people seeking loans for business ventures or education?\n\n\nCode\n# loan intent\nanalyze_loan_intent = df_test.groupby('loan_intent').agg({\n    'approved': 'mean',\n    'loan_status': 'mean'  # Actual default rate\n}).reset_index()\nanalyze_loan_intent['approval_rate'] = analyze_loan_intent['approved'] * 100\nanalyze_loan_intent['default_rate'] = analyze_loan_intent['loan_status'] * 100\nprint(analyze_loan_intent[['loan_intent', 'approval_rate', 'default_rate']])\n\n\n         loan_intent  approval_rate  default_rate\n0  DEBTCONSOLIDATION      90.265487     28.761062\n1          EDUCATION      92.346939     16.751701\n2    HOMEIMPROVEMENT      96.103896     25.000000\n3            MEDICAL      89.655172     28.424977\n4           PERSONAL      91.182365     22.044088\n5            VENTURE      91.804979     14.626556\n\n\nHere we see the approval and default rates based on loan intent. Looking at the approval rates, medical loans do in fact have the lowest approval rate and the second highest actual rate of default. On the other hand, education hsa the second highest approval rate and the third highest actual rate of default. Venture loans have a similar approval rate but the lowest default rate.\n\n\nCode\n# Loan intent plot\nplt.bar(analyze_loan_intent['loan_intent'], analyze_loan_intent['approval_rate'], color='lightgreen')\nplt.axhline(y=df_test['approved'].mean() * 100, color='r', linestyle='--', label='Overall')\nplt.title('Approval Rate by Loan Purpose')\nplt.xlabel('Loan Purpose')\nplt.ylabel('Approval Rate (%)')\nplt.xticks(rotation=45)\nplt.legend()\n\n\n\n\n\n\n\n\n\nThe plot above helps visualize the numerical values above. The red line shows the overall approval rate, and we see how different intents compare to that overall standard. To answer the question, different intents do indeed have different approval rates.\n\n\nHow does a person’s income level impact the ease with which they can access credit under your decision system?\n\n\nCode\n# 3. Analyze by income level\ndf_test['income_category'] = pd.qcut(df_test['person_income'], \n                                    q=5, \n                                    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\nincome_analysis = df_test.groupby('income_category').agg({\n    'approved': 'mean',\n    'loan_status': 'mean'  # Actual default rate\n}).reset_index()\nincome_analysis['approval_rate'] = income_analysis['approved'] * 100\nincome_analysis['default_rate'] = income_analysis['loan_status'] * 100\nprint(income_analysis[['income_category', 'approval_rate', 'default_rate']])\nprint(f\"Overall approval rate: {df_test['approved'].mean() * 100:.1f}%\")\n\n\n  income_category  approval_rate  default_rate\n0        Very Low      80.625543     43.527368\n1             Low      87.403599     23.736075\n2          Medium      94.469224     18.822480\n3            High      96.163906     14.995641\n4       Very High      99.650655     10.131004\nOverall approval rate: 91.6%\n\n\nI created 5 income brackets to help show which segments of perspective borrowers have access to credit. Clearly wealth plays a role: the very high income bracket almost has a 100% approval rate, and the high bracket is not far behind. Going to the very low bracket, it is much lower (80%). It is important though to look at the actual default rates. While the approval rate for very low income is 20% lower than the very high income bracket, the actual default rate is notably 33% higher. So, the approval rate differs logically, except it is interesting how the gap between approval rates and actual default rates is not entirely linear.\n\n\nCode\nplt.bar(income_analysis['income_category'], income_analysis['approval_rate'], color='salmon')\nplt.axhline(y=df_test['approved'].mean() * 100, color='r', linestyle='--', label='Overall')\nplt.title('Approval Rate by Income Level')\nplt.xlabel('Income Level')\nplt.ylabel('Approval Rate (%)')\nplt.xticks(rotation=45)\nplt.legend()\n\n\n\n\n\n\n\n\n\nA simple plot to display how approval rates differ by income level, compared to the overall approval rate.\n\n\nConcluding Thoughts\nTo goal of this blog post was to explore the impact of a purely profit-driven model on access to credit. So, I made an algorithm that works to make the most money for a bank. Consequently, I used the “best” features for modeling loan status and set a threshold value that offered loans to very few people – only those rated with a very low chance of default. These people, unsurprisingly, tended to have high income, be younger in age, and higher success getting a loan for home improvement than medical use. This is blatantly concerning and really delves into the deeper values of aid and capitalism.\nThe people the the hypothetical bank is offering loans to are likely not in need of the loans as badly as the others. A home improvement project by a higher income individual seems magnitudially less crucial than a medical necessity.\nConsidering that people seeking loans for medical expenses have high rates of default, it may seem financially prudent to restrict their access to credit. However, this raises important ethical questions about whether our financial systems should prioritize profit over human welfare, especially when the need stems from unavoidable health crises rather than discretionary spending. So, in my opinion, I strongly value health over profit and thus the weight that we put on the probability of defualting should be lowered when regarding human welfare to give more loan access for medical needs.\nUltimately, this highlights the necessity to define what fairness and equity means in the context of model building. This sheds light on the ethics of model creating an algorithm by defining weights to features that are likely already inherently affected by systemic biases."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Pretty Lit & Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Design and Impact of Automated Decision Systems\n\n\n\n\n\nDelve into a profit-driven loan approval model. Explore its ethical implications.\n\n\n\n\n\nMar 2, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nThis blog post explores the Palmer Penguins dataset to develop a classification model that can identify penguin species based on their characteristics.\n\n\n\n\n\nFeb 19, 2025\n\n\nAndrew Dean\n\n\n\n\n\n\nNo matching items"
  }
]