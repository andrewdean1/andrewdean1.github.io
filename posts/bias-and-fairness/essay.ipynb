{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Limits of the Quantitative Approach to Bias and Fairness\n",
    "author: Andrew Dean\n",
    "date: '2025-03-26'\n",
    "image: \"essay.gif\"\n",
    "description: \"Disccusses, considers, and critiques the the methods used to ensure fairness in ML algorithms.\"\n",
    "code-fold: true\n",
    "bibliography: refs.bib\n",
    "execute:\n",
    "    warning: false\n",
    "    message: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data now fuels many aspects of daily life. Digital technologies continually record our activities using structured formats such as databases and JSON files from social media. Rapid advances in data processing mean that large datasets are easily and quickly retrieved via APIs for analysis. Machine learning systems use this information to make decisions and recommendations, often without users’ explicit awareness. For industries, governments, and academic researchers, these models are indispensable - but they have also exhibited systemic biases. As algorithmic decision-making expanded into consequential domains, critics have identified biases against specific demographic groups. Notable examples that we have discussed include predictive policing algorithms and criminal recidivism tools that were demonstrated to exhibit significant bias against racial minorities. Because of this, it is crucial to employ metrics that assess algorithmic fairness. Today, various fairness verification frameworks are widely used, employing various quantitative methodologies, but this does not mean algorithms are thus definitively fair. Before we get into that, let's discuss what fairness truly means in the machine learning realm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three primary quantitative fairness definitions exist. Error rate parity ensures all groups experience identical false negative and false positive rates - meaning algorithmic mistakes occur with equal frequency regardless of whether an individual belongs to Group A or Group B. Acceptance rate parity equalizes acceptance rates across all groups, ensuring algorithmic outcomes remain independent of group membership. Sufficiency requires that the probability of experiencing a positive outcome following a positive prediction remains consistent across all subgroups while similarly ensuring the probability of experiencing a negative outcome following a negative prediction remains uniform across subgroups. Another key notion, demographic parity, mandates equal selection rates across groups, aligning with the moral perspective of distributive justice by aiming to prevent systemic disadvantage. However, as Barocas, Hardt, and Narayanan (@barocasFairnessMachineLearning2023) note, this approach can overlook differences in qualification rates, raising questions about its applicability in ensuring true fairness.\n",
    "\n",
    "From a moral perspective, three viewpoints frame fairness considerations: narrow, middle, and broad. The narrow view maintains that individuals similar in task-relevant aspects deserve similar treatment, comparing all people as individuals rather than as group members. The middle view says that decision-makers must avoid perpetuating injustice by treating apparently dissimilar individuals similarly when their differences stem from problematic origins. The broad equality view aspires for individuals with comparable abilities and ambitions to achieve similar successes despite inevitable inequalities. This perspective transcends decision-making fairness to address the fundamental design of societal institutions, aiming to prevent unjust disparities from emerging initially (@barocasFairnessMachineLearning2023).\n",
    "\n",
    "A real-world example illustrating the challenges of quantitative fairness is the use of the COMPAS algorithm in the U.S. judicial system to predict recidivism. A study by ProPublica found that Black defendants were disproportionately labeled as higher risk compared to white defendants, despite similar reoffending rates (@angwinFairnessMachineLearning2016). This case highlights the difficulties in achieving error rate parity, as false positive and false negative rates were not equal across racial groups. From a moral standpoint, this aligns with the middle fairness perspective, which emphasizes the importance of addressing systemic biases that lead to unjust outcomes (@barocasFairnessMachineLearning2023).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "​An illustrative example of employing both quantitative and moral frameworks to critique algorithmic bias is found in the study \"Algorithmic Bias? An Empirical Study into Apparent Gender-Based Discrimination in Displaying STEM Career Ads\" by Lambrecht and Tucker (@lambrechtAlgorithmicBiasEmpirical2019). This research investigates whether online advertising algorithms display STEM job advertisements differently based on gender. The authors conducted a field experiment by creating ads for STEM careers and observed their delivery across users. Their findings revealed that, even when controlling for factors like user behavior and advertiser intent, women were less likely to be shown these ads compared to men. The study suggests that this disparity arises not from explicit gender bias in the algorithms themselves but from underlying economic factors that lead to unintended, uneven outcomes. For instance, if women are less likely to click on STEM ads, the algorithm may optimize ad delivery towards men to maximize engagement, inadvertently perpetuating gender imbalances in STEM fields. This scenario demonstrates a failure to uphold the principle of demographic parity, where the likelihood of viewing a STEM career advertisement should be independent of gender. From a broader ethical standpoint, the study highlights the responsibility of platforms to ensure that their algorithms do not reinforce societal stereotypes or existing disparities. By critically analyzing and addressing these biases, the research underscores the importance of integrating both quantitative assessments and moral considerations to foster fairness in algorithmic decision-making.​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narayanan’s View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative definitions of fairness alone are insufficient to exonerate an algorithm, argues Arvind Narayanan, a computer scientist and professor at Princeton University. In his October 11th, 2022 talk, titled *“The Limits of the Quantitative Approach to Discrimination,”* Narayanan presents a critical perspective on the overreliance on classical numerical fairness metrics. His key assertion highlights this: “Currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (@narayananLimitsQuantitativeApproach2022). \n",
    "\n",
    "He opens his speech with a well known quote: “all models are wrong, but some models are useful” (@narayananLimitsQuantitativeApproach2022), emphasizing that no machine learning model is perfectly accurate. The very process of simplifying complex realities into data-friendly trends inevitably introduces bias. By relying on these generalizations, models unintentionally reinforce the existing state of the world - a state already riddled with inequality and discrimination. Narayanan points out that data is far from neutral, stating, “data aren’t inert and objective. They are political, and produced towards certain ends” (@narayananLimitsQuantitativeApproach2022). Since data collection serves specific purposes, bias is inherently woven into the dataset from the start. \n",
    "\n",
    "This idea is further elaborated in *Fairness and Machine Learning: Limitations and Opportunities,* co-authored by Narayanan, Barocas, and Hardt. The authors explain how the world’s complexities are distilled into rows, columns, and numerical values — an inherently flawed process. They challenge the notion of data as an objective snapshot of reality, arguing that “the term measurement is misleading, evoking an image of a dispassionate scientist recording what she observes, yet…it requires subjective human decisions” (@barocasFairnessMachineLearning2023). \n",
    "\n",
    "While people often believe data speaks for itself, Narayanan offers a more clear cut view: data is less an impartial truth and more a reflection of the systemic biases and inequalities that shape society. To counteract this, he advocates for a shift in focus: “we should be spending most of our time on curating and interrogating datasets before ever searching for statistical significance or fitting a model … it is important to look behind the facade of numbers to understand the hidden assumptions and politics of datasets” (@narayananLimitsQuantitativeApproach2022). This approach essentially reframes the role of data analysis, urging researchers to prioritize understanding the dataset’s origins and embedded biases rather than rushing to derive conclusions from potentially flawed numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Quantitative Method: Falling Short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arvind Narayanan's critique underscores the limitations of relying solely on quantitative methods to assess algorithmic fairness. In Bokányi and Hannák’s study, \"Ride-share matching algorithms generate income inequality\" (@bokanyiRideshareMatchingAlgorithms2021), the authors use a detailed computational model to illustrate how small changes in parameters - such as taxi density, demand-to-supply ratios, spatial distributions of trips, and driver idling strategies — can lead to considerable and unpredictable income disparities among drivers with identical work efforts. While the study rigorously exposes how algorithm design choices can trigger significant wage gaps through feedback loops, relying exclusively on these quantitative findings can mislead platform designers. If designers assumed that optimizing these numerical metrics fully defines their responsibilities, they might neglect broader ethical concerns that go far beyond optimizing these said unequal measurements. For example, if drivers still suspect that some inequality is present, if the platform designers think their algorithm is now perfect because they \"solved\" its prior flaws, they may disregard any further questioning on their platoform being fair. This narrow focus could prompt decisions that, for example, prioritize short-term efficiency and income equality while overlooking long-term structural inequities, ultimately reinforcing systemic biases. An example of this could be a platform solely optimizing its algorithm to balance daily driver incomes by assigning more rides to those with lower earnings; while this might yield fairer distributions on paper in the short run, it could inadvertently relegate drivers in underserved areas to persistently low-earning regions, thereby deepening geographic income disparities and perpetuating long-term structural inequities. In effect, by omitting the perspective that includes ethical and socio-economic implications, platform designers risk making poor decisions that perpetuate vulnerabilities among drivers. Thus, while the study is invaluable in quantifying disparities, its omission of systemic and ethical dimensions could lead to harmful, one-dimensional interventions if taken as a comprehensive guideline for algorithmic responsibility.\n",
    "\n",
    "This study exemplifies what D’Ignazio and Klein (@dignazioDataFeminism2023) term “Big Dick Data” in Data Feminism. They define this concept as big data projects driven by patriarchal, controlling ambitions of totalizing control through data collection and analysis. These projects are particularly harmful because they disregard context and overstate their technical and scientific authority. Data Feminism emphasizes the necessity of interrogating the the platforms under which data is gathered, an essential step that Ali et al. blatantly neglected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tie it back to Narayanan's speech, he opens it up with a statement that should not be taken lightly: “I hope that this talk goes some way towards busting the myth that numbers don’t lie” (@narayananLimitsQuantitativeApproach2022). By the end, it’s clear that numbers, while often treated as objective truth, are shaped by the biases baked into the data they’re drawn from. This challenges the common belief that many, including myself, have in data as a driving force of truth. Through exploring these studies, it becomes evident that datasets are never fully neutral or detached from the world’s inequalities, and should thus all be taken from a critical perspective. As seen in the studies I have discussed, researchers can manipulate quantitative methods to validate their desired outcomes and frame the results in a way that is favorable to their argument.  \n",
    "\n",
    "Narayanan’s argument pushes even further, claiming that quantitative methods often do more harm than good, reinforcing systemic biases under the notion of objectivity. While this criticism holds weight, I believe the solution isn’t to abandon quantitative methods altogether but to recognize their limitations and use them more thoughtfully. Quantitative metrics can still be powerful tools to uncover injustice if used responsibly. The key lies in avoiding reliance on any single definition of fairness. Since different definitions can yield conflicting conclusions, multiple perspectives must be considered to paint a fuller, more honest picture of algorithmic behavior. Moreover, numbers alone are never enough. Ethical frameworks, including the narrow, middle, and broad views of fairness, must guide how we interpret and apply these metrics. Only by blending quantitative analysis with moral reasoning can we hope to uncover and challenge the hidden biases within algorithmic systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
